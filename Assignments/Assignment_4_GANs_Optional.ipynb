{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignment  - GANs for Image Generation\n",
    "\n",
    "**Due April 5, 2020**\n",
    "\n",
    "### This is an optional assignment. The best 3 of 4 assignment grades will be used ###  \n",
    "\n",
    "You MUST do this assignment alone. You can use TensorFlow, Keras, PyTorch over whatever you want.\n",
    "\n",
    "### Part A - GANs Deep Learning model A (40 points)   \n",
    "\n",
    "Create a GANs generator for some data (images, fonts, etc.).  You cannot use the same data as another student. If using a notebook from the Internet you cannot use the default idata with the notebook. The TAs will set up a data sign-up sheet. \n",
    "\n",
    "Explain how the GAN works. Did this GAN work well. Why or why not?  Which hyperparameters are most important? Why?  Back up your reasoning with data.\n",
    "\n",
    "### Part B - GANs Deep Learning model B (40 points)   \n",
    "\n",
    "Create a GANs generator for the same data in part A using a different kind of GAN from part A. \n",
    "\n",
    "Explain how the GAN works. Did this GAN work well. Why or why not?  Which hyperparameters are most important? Why?  Back up your reasoning with data.? A or B? Why?  \n",
    "\n",
    "Which GAN worked better\n",
    "\n",
    "\n",
    "### Part C - Professionalism  (20 points)   \n",
    "\n",
    "\n",
    "It MUST run. (5 Points)\n",
    "The code must run on a laptop other than yours. There MUST be a clear README on how to run it.   \n",
    "\n",
    "\n",
    "What code is yours and what have you adapted and licensing? (5 Points)\n",
    "You must explain what code you wrote and what you have done that is different. Failure to cite ANY code will result in a zero for this section.  Did I explain my licensing clearly? Failure to cite a clear license will result in a zero for this section.   \n",
    "\n",
    "\n",
    "Did I explain my code clearly? (10 Points) Your code review score will be scaled to a range of 0 to 10 and be used for this score.   \n",
    "\n",
    "\n",
    "## GAN papers\n",
    "+ Generative Adversarial Networks, [[paper]](https://arxiv.org/abs/1406.2661), [[github]](https://github.com/goodfeli/adversarial)\n",
    "+ Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1511.06434), [[github]](https://github.com/soumith/dcgan.torch)\n",
    "+ Improved Techniques for Training GANs, [[paper]](https://arxiv.org/pdf/1606.03498.pdf), [[github]](https://github.com/openai/improved-gan)\n",
    "+ BEGAN: Boundary Equilibrium Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1703.10717), [[github]](https://github.com/carpedm20/BEGAN-tensorflow)\n",
    "\n",
    "\n",
    "\n",
    "## GAN tutorials with easy and simple example code for starters\n",
    "+ [1D Generative Adversarial Network Demo](http://notebooks.aylien.com/research/gan/gan_simple.html)\n",
    "+ [starter from \"How to Train a GAN?\" at NIPS2016](https://github.com/soumith/ganhacks)\n",
    "+ [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)\n",
    "+ [OpenAI - Generative Models](https://blog.openai.com/generative-models/)\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "+ [Applications using GANs](#applications-using-gans)\n",
    "    + [Font generation](#font-generation)\n",
    "    + [Anime character generation](#anime-character-generation)\n",
    "    + [Text2Image (text to image)](#text2image-text-to-image)\n",
    "    + [3D Object generation](#3d-object-generation)\n",
    "    + [Image Editing](#image-editing)\n",
    "    + [Domain-transfer (e.g. style-transfer, pix2pix, sketch2image)](#domain-transfer-eg-style-transfer-pix2pix-sketch2image)\n",
    "    + [Image Inpainting (hole filling)](#image-inpainting-hole-filling)\n",
    "    + [Super-resolution](#super-resolution)\n",
    "    + [High-resolution image generation (large-scale image)](#high-resolution-image-generation-large-scale-image)\n",
    "    + [Visual Saliency Prediction (attention prediction)](#visual-saliency-prediction-attention-prediction)\n",
    "    + [Synthetic Data Generation](#synthetic-data-generation)\n",
    "    + [Photorealistic Image generation (e.g. pix2pix, sketch2image)](#photorealistic-image-generation-eg-pix2pix-sketch2image)\n",
    "    + [Human Pose Estimation](#human-pose-estimation-1)\n",
    "    + [3D Object generation](#3d-object-generation-1)\n",
    "\n",
    "\n",
    "## Applications using GANs\n",
    "\n",
    "### Font generation\n",
    "+ Learning Chinese Character style with conditional GAN, [[blog]](https://kaonashi-tyc.github.io/2017/04/06/zi2zi.html), [[github]](https://github.com/kaonashi-tyc/zi2zi)\n",
    "\n",
    "### Anime character generation\n",
    "+ Towards the Automatic Anime Characters Creation with Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1708.05509)\n",
    "+ [Project] A simple PyTorch Implementation of Generative Adversarial Networks, focusing on anime face drawing, [[github]](https://github.com/jayleicn/animeGAN)\n",
    "+ [Project] A simple, clean TensorFlow implementation of Generative Adversarial Networks with a focus on modeling illustrations, [[github]](https://github.com/tdrussell/IllustrationGAN)\n",
    "+ [Project] Keras-GAN-Animeface-Character, [[github]](https://github.com/forcecore/Keras-GAN-Animeface-Character)\n",
    "+ [Project] A DCGAN to generate anime faces using custom mined dataset, [[github]](https://github.com/pavitrakumar78/Anime-Face-GAN-Keras)\n",
    "\n",
    "\n",
    "### Text2Image (text to image)\n",
    "+ TAC-GAN â€“ Text Conditioned Auxiliary Classifier Generative Adversarial Network, [[paper]](https://arxiv.org/pdf/1703.06412.pdf), [[github]](https://github.com/dashayushman/TAC-GAN)\n",
    "+ StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1612.03242.pdf), [[github]](https://github.com/hanzhanggit/StackGAN)\n",
    "+ Generative Adversarial Text to Image Synthesis, [[paper]](https://arxiv.org/pdf/1605.05396.pdf), [[github]](https://github.com/paarthneekhara/text-to-image), [[github]](https://github.com/reedscot/icml2016)\n",
    "+ Learning What and Where to Draw, [[paper]](http://www.scottreed.info/files/nips2016.pdf), [[github]](https://github.com/reedscot/nips2016)\n",
    "\n",
    "### 3D Object generation\n",
    "+ Parametric 3D Exploration with Stacked Adversarial Networks, [[github]](https://github.com/maxorange/pix2vox), [[youtube]](https://www.youtube.com/watch?v=ITATOXVvWEM)\n",
    "+ Learning a Probabilistic Latent Space of Object\n",
    "Shapes via 3D Generative-Adversarial Modeling, [[paper]](http://papers.nips.cc/paper/6096-learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling.pdf), [[github]](https://github.com/zck119/3dgan-release), [[youtube]](https://www.youtube.com/watch?v=HO1LYJb818Q)\n",
    "+ 3D Shape Induction from 2D Views of Multiple Objects, [[paper]](https://arxiv.org/pdf/1612.05872.pdf)\n",
    "+ Fully Convolutional Refined Auto-Encoding Generative Adversarial Networks for 3D Multi Object Scenes, [[github]](https://github.com/yunishi3/3D-FCR-alphaGAN), [[blog]](https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80)\n",
    "\n",
    "### Image Editing\n",
    "+ Invertible Conditional GANs for image editing, [[paper]](https://arxiv.org/abs/1611.06355), [[github]](https://github.com/Guim3/IcGAN)\n",
    "+ Image De-raining Using a Conditional Generative Adversarial Network, [[paper]](https://arxiv.org/abs/1701.05957), [[github]](https://github.com/hezhangsprinter/ID-CGAN)\n",
    "\n",
    "\n",
    "### Domain-transfer (e.g. style-transfer, pix2pix, sketch2image)\n",
    "+ Image-to-Image Translation with Conditional Adversarial Networks, [[paper]](https://arxiv.org/pdf/1611.07004), [[github]](https://github.com/phillipi/pix2pix), [[youtube]](https://www.youtube.com/watch?v=VVqxbmUJorQ)\n",
    "+ Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, [[paper]](https://arxiv.org/pdf/1703.10593.pdf), [[github]](https://github.com/junyanz/CycleGAN), [[youtube]](https://www.youtube.com/watch?v=JzgOfISLNjk)\n",
    "+ Learning to Discover Cross-Domain Relations with Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1703.05192.pdf), [[github]](https://github.com/carpedm20/DiscoGAN-pytorch)\n",
    "+ Unsupervised Creation of Parameterized Avatars, [[paper]](https://arxiv.org/pdf/1704.05693.pdf)\n",
    "+ UNSUPERVISED CROSS-DOMAIN IMAGE GENERATION, [[paper]](https://openreview.net/pdf?id=Sk2Im59ex)\n",
    "+ Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks, [[paper]](http://arxiv.org/abs/1604.04382), [[github]](https://github.com/chuanli11/MGANs)\n",
    "+ Pixel-Level Domain Transfer  [[paper]](https://arxiv.org/pdf/1603.07442), [[github]](https://github.com/fxia22/PixelDTGAN)\n",
    "+ TextureGAN: Controlling Deep Image Synthesis with Texture Patches, [[paper]](https://arxiv.org/pdf/1706.02823.pdf), [[demo]](https://github.com/varunagrawal/t-gan-demo)\n",
    "+ Vincent AI Sketch Demo Draws In Throngs at GTC Europe, [[blog](https://blogs.nvidia.com/blog/2017/10/11/vincent-ai-sketch-demo-draws-in-throngs-at-gtc-europe/)], [[youtube]](https://www.youtube.com/watch?v=kIcqXTUMwps)\n",
    "+ Deep Photo Style Transfer, [[paper]](https://arxiv.org/pdf/1703.07511.pdf), [[github]](https://github.com/luanfujun/deep-photo-styletransfer)\n",
    "\n",
    "\n",
    "### Image Inpainting (hole filling)\n",
    "+ Context Encoders: Feature Learning by Inpainting, [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf), [[github]](https://github.com/pathak22/context-encoder)\n",
    "+ Semantic Image Inpainting with Perceptual and Contextual Losses, [[paper]](https://arxiv.org/abs/1607.07539), [[github]](https://github.com/bamos/dcgan-completion.tensorflow)\n",
    "+ SEMI-SUPERVISED LEARNING WITH CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS, [[paper]](https://arxiv.org/pdf/1611.06430v1.pdf)\n",
    "+ Generative Face Completion, [[paper]](https://drive.google.com/file/d/0B8_MZ8a8aoSeenVrYkpCdnFRVms/edit), [[github]](https://github.com/Yijunmaverick/GenerativeFaceCompletion)\n",
    "\n",
    "### Super-resolution\n",
    "+ Image super-resolution through deep learning, [[github]](https://github.com/david-gpu/srez)\n",
    "+ Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, [[paper]](https://arxiv.org/abs/1609.04802), [[github]](https://github.com/leehomyc/Photo-Realistic-Super-Resoluton)\n",
    "+ High-Quality Face Image Super-Resolution Using Conditional Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1707.00737.pdf)\n",
    "+ Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network, [[paper]](https://arxiv.org/pdf/1811.00344.pdf), [[github]](https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw)\n",
    "\n",
    "\n",
    "\n",
    "### High-resolution image generation (large-scale image)\n",
    "+ Generating Large Images from Latent Vectors, [[blog]](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/), [[github]](https://github.com/hardmaru/cppn-gan-vae-tensorflow)\n",
    "+ PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION, [[paper]](http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of//karras2017gan-paper.pdf), [[github]](https://github.com/tkarras/progressive_growing_of_gans)\n",
    "\n",
    "\n",
    "\n",
    "### Visual Saliency Prediction (attention prediction)\n",
    "+ SalGAN: Visual Saliency Prediction with Generative Adversarial Networks, [[paper]](https://arxiv.org/pdf/1701.01081), [[github]](https://github.com/imatge-upc/saliency-salgan-2017)\n",
    "\n",
    "\n",
    "### Synthetic Data Generation\n",
    "+ Learning from Simulated and Unsupervised Images through Adversarial Training, [[paper]](https://arxiv.org/pdf/1612.07828.pdf), [[github]](https://github.com/carpedm20/simulated-unsupervised-tensorflow)\n",
    "\n",
    "\n",
    "### Super-resolution\n",
    "+ Learning to Simplify:\n",
    "Fully Convolutional Networks for Rough Sketch Cleanup, [[paper]](http://delivery.acm.org/10.1145/2930000/2925972/a121-simo-serra.pdf?ip=111.91.137.238&id=2925972&acc=ACTIVE%20SERVICE&key=58C7DD92F91E3631%2E58C7DD92F91E3631%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=818332500&CFTOKEN=94661101&__acm__=1507786813_0e5b28dfb97e654d0126d61b0aa592f4), [[site link]](http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/), [[youtube]](https://www.youtube.com/watch?v=4MfG9CDufPA)\n",
    "\n",
    "### Photorealistic Image generation (e.g. pix2pix, sketch2image)\n",
    "+ The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies, [[paper]](http://delivery.acm.org/10.1145/2930000/2925954/a119-sangkloy.pdf?ip=111.91.137.238&id=2925954&acc=CHORUS&key=58C7DD92F91E3631%2E58C7DD92F91E3631%2E4D4702B0C3E38B35%2E6D218144511F3437&CFID=818332500&CFTOKEN=94661101&__acm__=1507787415_cb950c300370fc27da68920a0d5b5178), [[youtube]](https://www.youtube.com/watch?v=a3sgFQjEfp4)\n",
    "+ PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing, [[paper]](https://www.researchgate.net/profile/Eli_Shechtman/publication/220184392_PatchMatch_A_Randomized_Correspondence_Algorithm_for_Structural_Image_Editing/links/02e7e520897b12bf0f000000.pdf), [[github]](https://github.com/younesse-cv/PatchMatch), [[youtube]](https://www.youtube.com/watch?v=n3aoc36V8LM)\n",
    "\n",
    "\n",
    "### 3D Object generation\n",
    "+ 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction, [[paper]](http://arxiv.org/abs/1604.00449), [[github]](https://github.com/chrischoy/3D-R2N2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2018\n",
    "\n",
    "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
