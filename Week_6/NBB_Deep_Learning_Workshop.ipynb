{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"Deep Learning?\"\n",
    "\n",
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is the stacking of artificial neural networks (ANNs) to create stacked neural networks, [deep belief networks](https://en.wikipedia.org/wiki/Deep_belief_network), [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) and deep generative models. A deep neural network (DNN) is an ANN with multiple hidden layers between the input and output layers.\n",
    "\n",
    "An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.\n",
    "\n",
    "_ Deep learning is basically the deep stacking of artificial neurons to learn complex models of data. _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Learning?\n",
    "\n",
    "- It works\n",
    "\n",
    "Deep learning and neural networks are increasingly important concepts as demonstrated through their performance on difficult problems in computer vision medical diagnosis, natural language processing and many other domains. \n",
    "\n",
    "- Learns feature selection   \n",
    "\n",
    "Deep learning algorithms are unique in that they try to learn latent features from data, as opposed to traditional machine learning where features selection is typically handcrafted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Success: Vision\n",
    "\n",
    "![Chihuahua or Muffin](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Dog_Muffins.jpg)\n",
    "\n",
    "\n",
    "**Detect pneumothorax in real X-Ray scans**\n",
    "\n",
    "![Detect pneumothorax in real X-Ray scans](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Detect_pneumothorax_in_real_X-Ray_scans.png)\n",
    "\n",
    "### Deep Learning Success: Audio  \n",
    "\n",
    "![Music_RBM](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Music_RBM.png)\n",
    "\n",
    "from [Music RBM](https://chelseapran4.wixsite.com/datascience)\n",
    "\n",
    "### Deep Learning Success: Generative Adversarial Networks (GANs) \n",
    "\n",
    "![GANS Text](http://nikbearbrown.com/YouTube/MachineLearning/IMG/GANS_Paper_Text.png)\n",
    "\n",
    "![Style Transfer GANs](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Style_Transfer_GANs.jpg)\n",
    "\n",
    "\n",
    "### Self-Driving Cars\n",
    "\n",
    "![Self-Driving Car Game](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Self-Driving_Car_Game.png)\n",
    "\n",
    "![Self Driving Car using CNN](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Self_Driving_Car_using_CNN.png)\n",
    "\n",
    "from [Self Driving Car using CNN](https://youtu.be/-zWhgLDXqyY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wDw67NOSXP+WQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12400ee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a 3-dimensional array of each image, and that images width and image height (28×28 pixels per image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "## 60K 28×28 sized training images\n",
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127766b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 4 more images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[55], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[555], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[5555], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[55555], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron (Neural units)\n",
    "\n",
    "**The Perceptron - The structural building block of deep learning**  \n",
    "\n",
    "The [perceptron]()https://en.wikipedia.org/wiki/Perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide\n",
    "whether an input, represented by a vector of numbers, belongs to some\n",
    "specific class or not). It is a type of linear classifier, i.e. a\n",
    "classification algorithm that makes its predictions based on a linear\n",
    "predictor function combining a set of weights with the feature\n",
    "vector. \n",
    "\n",
    "**The Perceptron: Forward Propagation**\n",
    "\n",
    "**The Perceptron: Forward Propagation**\n",
    "\n",
    "![The Perceptron: Forward Propagation](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron.png)\n",
    "\n",
    "Slide from [6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/)  \n",
    "\n",
    "![The Perceptron: Forward Propagation](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron_2.png)\n",
    "\n",
    "Slide from [6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/) \n",
    "\n",
    "![The Perceptron: Forward Propagation](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron_3.png)\n",
    "\n",
    "Slide from [6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/) \n",
    "\n",
    "![The Perceptron: Forward Propagation](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron_4.png)\n",
    "\n",
    "Slide from [6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/) \n",
    "The perceptron algorithm dates back to the late 1950s, and is the basis of [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
    "\n",
    "Definition\n",
    "----------\n",
    "\n",
    "In the modern sense, the perceptron is an algorithm for learning a\n",
    "binary classifier: a function that maps its input (a real-valued\n",
    "[vector]) to an output value $f(x)$ (a single [binary] value):\n",
    "\n",
    "$$f(x) = \\begin{cases}1 & \\text{if }\\ w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "where is a vector of real-valued weights, $w \\cdot x$ is the [dot\n",
    "product] $\\sum_{i=1}^m w_i x_i$, where m is the number of inputs to the\n",
    "perceptron and is the *bias*. The bias shifts the decision boundary away\n",
    "from the origin and does not depend on any input value.\n",
    "\n",
    "The value of $f(x)$ (0 or 1) is used to classify as either a positive or\n",
    "a negative instance, in the case of a binary classification problem. If\n",
    "is negative, then the weighted combination of inputs must produce a\n",
    "positive value greater than $|b|$ in order to push the classifier neuron\n",
    "over the 0 threshold. Spatially, the bias alters the position (though\n",
    "not the orientation) of the decision boundary. The perceptron learning\n",
    "algorithm does not terminate if the learning set is not linearly\n",
    "separable. If the vectors are not linearly separable learning will\n",
    "never reach a point where all vectors are classified properly. The most\n",
    "famous example of the perceptron's inability to solve problems with\n",
    "linearly nonseparable vectors is the Boolean exclusive-or problem. The\n",
    "solution spaces of decision boundaries for all binary functions and\n",
    "learning behaviors are studied in the reference.\n",
    "\n",
    "In the context of neural networks, a perceptron is an _artificial\n",
    "neuron_ using the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) as the activation function.\n",
    "The perceptron algorithm is also termed the **single-layer perceptron**,\n",
    "to distinguish it from a multilayer perceptron, which is a misnomer\n",
    "for a more complicated neural network. As a linear classifier, the\n",
    "single-layer perceptron is the simplest [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Learning algorithm\n",
    "------------------\n",
    "\n",
    "Below is an example of a learning algorithm for a (single-layer)\n",
    "perceptron. For [multilayer perceptrons], where a hidden layer exists,\n",
    "more sophisticated algorithms such as [backpropagation] must be used.\n",
    "Alternatively, methods such as the [delta rule] can be used if the\n",
    "function is non-linear and differentiable, although the one below will\n",
    "work as well.\n",
    "\n",
    "When multiple perceptrons are combined in an artificial neural network,\n",
    "each output neuron operates independently of all the others; thus,\n",
    "learning each output can be considered in isolation. ![A diagram showing a perceptron updating its linear boundary as more\n",
    "training examples are added.](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron_example.svg.png)\n",
    "\n",
    "### Definitions\n",
    "\n",
    "We first define some variables:\n",
    "\n",
    "-   $y = f(\\mathbf{z})$ denotes the *output* from the perceptron for an\n",
    "    input vector $\\mathbf{z}$.\n",
    "-   $D = \\{(\\mathbf{x}_1,d_1),\\dots,(\\mathbf{x}_s,d_s)\\}$ is the\n",
    "    *training set* of $s$ samples, where:\n",
    "    -   $\\mathbf{x}_j$ is the $n$-dimensional input vector.\n",
    "    -   $d_j$ is the desired output value of the perceptron for that\n",
    "        input.\n",
    "\n",
    "We show the values of the features as follows:\n",
    "\n",
    "-   $x_{j,i}$ is the value of the $i$th feature of the $j$th training\n",
    "    *input vector*.\n",
    "-   $x_{j,0} = 1$.\n",
    "\n",
    "To represent the weights:\n",
    "\n",
    "-   $w_i$ is the $i$th value in the *weight vector*, to be multiplied by\n",
    "    the value of the $i$th input feature.\n",
    "-   Because $x_{j,0} = 1$, the $w_0$ is effectively a bias that we use\n",
    "    instead of the bias constant $b$.\n",
    "\n",
    "To show the time-dependence of $\\mathbf{w}$, we use:\n",
    "\n",
    "-   $w_i(t)$ is the weight $i$ at time $t$.\n",
    "\n",
    "Unlike other linear classification algorithms such as [logistic\n",
    "regression], there is no need for a *learning rate* in the perceptron\n",
    "algorithm. This is because multiplying the update by any constant simply\n",
    "rescales the weights but never changes the sign of the prediction.\n",
    "\n",
    "![The appropriate weights are applied to the inputs, and the resulting\n",
    "weighted sum passed to a function that produces the output o.](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron.svg.png)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Initialize the weights and the threshold. Weights may\n",
    "be initialized to 0 or to a small random value. In the example below, we\n",
    "use 0.  \n",
    "2. For each example in our training set , perform the following\n",
    "steps over the input $\\mathbf{x}_j$ and desired output $d_j$   \n",
    "\n",
    "Calculate the actual output:   \n",
    "\n",
    "a. $$\\begin{align}\n",
    "y_j(t) &= f[\\mathbf{w}(t)\\cdot\\mathbf{x}_j] \\\\\n",
    "&= f[w_0(t)x_{j,0} + w_1(t)x_{j,1} + w_2(t)x_{j,2} + \\dotsb + w_n(t)x_{j,n}]\n",
    "\\end{align}$$\n",
    "\n",
    "b. Update the weights:  \n",
    "\n",
    "$$w_i(t+1) = w_i(t) + (d_j - y_j(t)) x_{j,i}$$, for all features\n",
    "$0 \\leq i \\leq n$.   \n",
    "\n",
    "**offline learning**\n",
    "\n",
    "For offline learning, the step 2 may be repeated until the iteration error  $\\frac{1}{s} \\sum_{j=1}^s |d_j - y_j(t)|$ is less than a user-specified error threshold $\\gamma$, or a predetermined number of iterations have been completed. The algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps. \n",
    "\n",
    "Multiclass perceptron\n",
    "---------------------\n",
    "\n",
    "Like most other techniques for training linear classifiers, the\n",
    "perceptron generalizes naturally to multiclass classification. Here,\n",
    "the input $x$ and the output $y$ are drawn from arbitrary sets. A\n",
    "feature representation function $f(x,y)$ maps each possible input/output\n",
    "pair to a finite-dimensional real-valued feature vector. As before, the\n",
    "feature vector is multiplied by a weight vector $w$, but now the\n",
    "resulting score is used to choose among many possible outputs:\n",
    "\n",
    "$$\\hat y = \\operatorname{argmax}_y f(x,y) \\cdot w.$$ ≈ Learning again\n",
    "iterates over the examples, predicting an output for each, leaving the\n",
    "weights unchanged when the predicted output matches the target, and\n",
    "changing them when it does not. The update becomes:\n",
    "\n",
    "$$w_{t+1} = w_t + f(x, y) - f(x,\\hat y).$$\n",
    "\n",
    "This multiclass feedback formulation reduces to the original perceptron\n",
    "when $x$ is a real-valued vector, $y$ is chosen from $\\{0,1\\}$, and\n",
    "$f(x,y) = y x$.\n",
    "\n",
    "For certain problems, input/output representations and features can be\n",
    "chosen so that $\\mathrm{argmax}_y f(x,y) \\cdot w$ can be found\n",
    "efficiently even though $y$ is chosen from a very large or even infinite\n",
    "set.\n",
    "\n",
    "In recent years, perceptron training has become popular in the field of\n",
    "[natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) for such tasks as [part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "and [syntactic parsing](https://en.wikipedia.org/wiki/Parsing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "In computational networks, the [activation function](https://en.wikipedia.org/wiki/Activation_function) of a node\n",
    "defines the output of that node given an input or set of inputs. A\n",
    "standard computer chip circuit can be seen as a digital network of\n",
    "activation functions that can be “ON” (1) or “OFF” (0), depending on\n",
    "input. This is similar to the behavior of the linear perceptron in\n",
    "neural networks. However, only *nonlinear* activation functions allow\n",
    "such networks to compute nontrivial problems using only a small number\n",
    "of nodes. In artificial neural networks this function is also called\n",
    "the **transfer function**.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "\n",
    "In biologically inspired neural networks, the activation function is\n",
    "usually an abstraction representing the rate of action potential\n",
    "firing in the cell. In its simplest form, this function is binary—that\n",
    "is, either the neuron is firing or not. The function looks like\n",
    "$\\phi(v_i)=U(v_i)$, where $U$ is the Heaviside step function. In this\n",
    "case many neurons must be used in computation beyond linear separation\n",
    "of categories.\n",
    "\n",
    "A line of positive slope may be used to reflect the increase in firing\n",
    "rate that occurs as input current increases. Such a function would be of\n",
    "the form $\\phi(v_i)=\\mu v_i$, where $\\mu$ is the slope. This activation\n",
    "function is linear, and therefore has the same problems as the binary\n",
    "function. In addition, networks constructed using this model have\n",
    "unstable convergence because neuron inputs along favored paths tend to\n",
    "increase without bound, as this function is not normalizable.\n",
    "\n",
    "All problems mentioned above can be handled by using a normalizable\n",
    "sigmoid activation function. One realistic model stays at zero until\n",
    "input current is received, at which point the firing frequency increases\n",
    "quickly at first, but gradually approaches an asymptote at 100% firing\n",
    "rate. Mathematically, this looks like $\\phi(v_i)=U(v_i)\\tanh(v_i)$,\n",
    "where the hyperbolic tangent function can be replaced by any sigmoid\n",
    "function. This behavior is realistically reflected in the neuron, as\n",
    "neurons cannot physically fire faster than a certain rate. This model\n",
    "runs into problems, however, in computational networks as it is not\n",
    "differentiable, a requirement to calculate backpropagation.\n",
    "\n",
    "The final model, then, that is used in multilayer perceptrons is a\n",
    "sigmoidal activation function in the form of a hyperbolic tangent. Two\n",
    "forms of this function are commonly used: $\\phi(v_i)=\\tanh(v_i)$ whose\n",
    "range is normalized from -1 to 1, and $\\phi(v_i) = (1+\\exp(-v_i))^{-1}$\n",
    "is vertically translated to normalize from 0 to 1. The latter model is\n",
    "often considered more biologically realistic, but it runs into\n",
    "theoretical and experimental difficulties with certain types. \n",
    "\n",
    "**Importance of Activation Functions**\n",
    "\n",
    "![Importance of Activation Functions](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Importance_of_Activation_Functions.png)\n",
    "\n",
    "Slide from [6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of activation functions\n",
    "\n",
    "Some desirable properties in an activation function include:\n",
    "\n",
    "-   Nonlinear – When the activation function is non-linear, then a\n",
    "    two-layer neural network can be proven to be a universal function\n",
    "    approximator. The identity activation function does not satisfy\n",
    "    this property. When multiple layers use the identity activation\n",
    "    function, the entire network is equivalent to a single-layer model.   \n",
    "-   Continuously differentiable – This property is necessary for\n",
    "    enabling gradient-based optimization methods. The binary step\n",
    "    activation function is not differentiable at 0, and it\n",
    "    differentiates to 0 for all other values, so gradient-based methods\n",
    "    can make no progress with it.   \n",
    "-   Range – When the range of the activation function is finite,\n",
    "    gradient-based training methods tend to be more stable, because\n",
    "    pattern presentations significantly affect only limited weights.\n",
    "    When the range is infinite, training is generally more efficient\n",
    "    because pattern presentations significantly affect most of the\n",
    "    weights. In the latter case, smaller learning rates are typically\n",
    "    necessary.   \n",
    "-   Monotonic – When the activation function is monotonic, the error\n",
    "    surface associated with a single-layer model is guaranteed to be\n",
    "    convex.   \n",
    "-   Smooth Functions with a Monotonic derivative – These have been shown\n",
    "    to generalize better in some cases. The argument for these\n",
    "    properties suggests that such activation functions are more\n",
    "    consistent with Occam's razor.   \n",
    "-   Approximates identity near the origin – When activation functions\n",
    "    have this property, the neural network will learn efficiently when\n",
    "    its weights are initialized with small random values. When the\n",
    "    activation function does not approximate identity near the origin,\n",
    "    special care must be used when initializing the weights.   \n",
    "\n",
    "## Rectified linear unit (ReLU) transfer function\n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "\n",
    "Activation identity \n",
    "\n",
    "$f(x)=x$   \n",
    "$f'(x)=1$   \n",
    "$(-\\infty,\\infty)$  \n",
    "$C^\\infty$   \n",
    "\n",
    "![Identity](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_identity.svg.png )\n",
    "\n",
    "\n",
    "Logistic (a.k.a. Soft step)\n",
    "\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$   \n",
    "$f'(x)=f(x)(1-f(x))$   \n",
    "$(0,1)$  \n",
    "$C^\\infty$  \n",
    "\n",
    "![Logistic](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_logistic.svg.png)  \n",
    "\n",
    "TanH\n",
    "\n",
    "$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$   \n",
    "$f'(x)=1-f(x)^2$    \n",
    "$(-1,1)$    \n",
    "$C^\\infty$    \n",
    "\n",
    "![TanH](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_tanh.svg.png)  \n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    0 & \\text{for } x < 0\\\\\n",
    "    x & \\text{for } x \\ge 0\\end{cases}$   \n",
    "    \n",
    "$f'(x) = \\begin{cases}\n",
    "    0 & \\text{for } x < 0\\\\\n",
    "    1 & \\text{for } x \\ge 0\\end{cases}$    \n",
    "\n",
    "$[0,\\infty)$   \n",
    "$C^0$  \n",
    "    \n",
    "![Rectified linear unit (ReLU)](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "\n",
    "The Rectified linear unit (ReLU) seem to work well empirically.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neural network\n",
    "\n",
    "[Artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) (**ANNs**) or **[connectionist] systems**\n",
    "are computing systems inspired by the biological neural networks that\n",
    "constitute animal brains. Such systems learn (progressively improve\n",
    "performance) to do tasks by considering examples, generally without\n",
    "task-specific programming. For example, in image recognition, they might\n",
    "learn to identify images that contain cats by analyzing example images\n",
    "that have been manually labeled as “cat” or “no cat” and using the\n",
    "analytic results to identify cats in other images. They have found most\n",
    "use in applications difficult to express in a traditional computer\n",
    "algorithm using rule-based programming.\n",
    "\n",
    "An ANN is based on a collection of connected units called artificial\n",
    "neurons, (analogous to axons in a biological brain). Each\n",
    "connection synapse) between neurons can transmit a signal to another\n",
    "neuron. The receiving (postsynaptic) neuron can process the signal(s)\n",
    "and then signal downstream neurons connected to it. Neurons may have\n",
    "state, generally represented by [real numbers], typically between 0 and 1.\n",
    "\n",
    "Neurons and synapses may also have a weight that varies as learning\n",
    "proceeds, which can increase or decrease the strength of the signal that\n",
    "it sends downstream. Further, they may have a threshold such that only\n",
    "if the aggregate signal is below (or above) that level is the downstream\n",
    "signal sent.\n",
    "\n",
    "Typically, neurons are organized in layers. Different layers may perform\n",
    "different kinds of transformations on their inputs. Signals travel from\n",
    "the first (input), to the last (output) layer, possibly after traversing\n",
    "the layers multiple times.\n",
    "\n",
    "The original goal of the neural network approach was to solve problems\n",
    "in the same way that a human brain would. Over time, attention focused\n",
    "on matching specific mental abilities, leading to deviations from\n",
    "biology such as backpropagation, or passing information in the reverse\n",
    "direction and adjusting the network to reflect that information.\n",
    "\n",
    "Neural networks have been used on a variety of tasks, including\n",
    "computer vision, speech recognition, machine translation, social\n",
    "network filtering, playing board and video games, medical diagnosis and\n",
    "in many other domains.\n",
    "\n",
    "**But what *is* a Neural Network?**\n",
    "\n",
    "![But what *is* a Neural Network?](http://nikbearbrown.com/YouTube/MachineLearning/IMG/But_what_is_a_Neural_Network.png)\n",
    "\n",
    "Slide from [But what *is* a Neural Network?](https://youtu.be/aircAruvnKk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "\n",
    "**Gradient descent** is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local\n",
    "minimum of a function using gradient descent, one takes steps\n",
    "proportional to the *negative* of the gradient (or of the approximate\n",
    "gradient) of the function at the current point. If instead one takes\n",
    "steps proportional to the *positive* of the gradient, one approaches a\n",
    "local maximum of that function; the procedure is then known as\n",
    "**gradient ascent**.\n",
    "\n",
    "Gradient descent is also known as **steepest descent**. However,\n",
    "gradient descent should not be confused with the method of steepest\n",
    "descent for approximating integrals.\n",
    "\n",
    "![Gradient ascent surface](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Gradient_ascent_surface.png)\n",
    "\n",
    "Gradient descent is based on the observation that if the [multi-variable\n",
    "function] $F(\\mathbf{x})$ is [defined] and [differentiable] in a\n",
    "neighborhood of a point $\\mathbf{a}$, then $F(\\mathbf{x})$ decreases\n",
    "*fastest* if one goes from $\\mathbf{a}$ in the direction of the negative\n",
    "gradient of $F$ at $\\mathbf{a}$, $-\\nabla F(\\mathbf{a})$. It follows\n",
    "that, if\n",
    "\n",
    "$$\\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n)$$\n",
    "\n",
    "for $\\gamma$ small enough, then\n",
    "$F(\\mathbf{a_n})\\geq F(\\mathbf{a_{n+1}})$. In other words, the term\n",
    "$\\gamma\\nabla F(\\mathbf{a})$ is subtracted from $\\mathbf{a}$ because we\n",
    "want to move against the gradient, toward the minimum. With this\n",
    "observation in mind, one starts with a guess $\\mathbf{x}_0$ for a local\n",
    "minimum of $F$, and considers the sequence\n",
    "$\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\dots$ such that\n",
    "\n",
    "$$\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$F(\\mathbf{x}_0)\\ge F(\\mathbf{x}_1)\\ge F(\\mathbf{x}_2)\\ge \\cdots,$$\n",
    "\n",
    "so hopefully the sequence $(\\mathbf{x}_n)$ converges to the desired\n",
    "local minimum. Note that the value of the *step size* $\\gamma$ is\n",
    "allowed to change at every iteration.\n",
    "\n",
    "**Gradient descent in python**\n",
    "~~~python \n",
    "cur_x = 6 # The algorithm starts at x=6\n",
    "gamma = 0.01 # step size multiplier\n",
    "precision = 0.00001\n",
    "previous_step_size = 1/precision; # some large value\n",
    "\n",
    "df = lambda x: 4 * x**3 - 9 * x**2\n",
    "\n",
    "while previous_step_size > precision:\n",
    "    prev_x = cur_x\n",
    "    cur_x += -gamma * df(prev_x)\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "\n",
    "print(\"The local minimum occurs at %f\" % cur_x)\n",
    "\n",
    "~~~\n",
    "\n",
    "![Gradient descent, how neural networks learn](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Gradient_descent.png)\n",
    "\n",
    "Slide from [Gradient descent, how neural networks learn](https://youtu.be/IHZwWFHWa-w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss or cost functions\n",
    "\n",
    "Loss function\n",
    "-------------\n",
    "\n",
    "Sometimes referred to as the **cost function** or **error function**\n",
    "(not to be confused with the Gauss error function), the loss function\n",
    "is a function that maps values of one or more variables onto a real\n",
    "number intuitively representing some “cost” associated with those\n",
    "values. For backpropagation, the loss function calculates the difference\n",
    "between the network output and its expected output, after a case\n",
    "propagates through the network.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Two assumptions must be made about the form of the error function.\n",
    "\n",
    "The first is that it can be written as an average\n",
    "$E=\\frac{1}{n}\\sum_xE_x$ over error functions $E_x$, for individual\n",
    "training examples, $x$. The reason for this assumption is that the\n",
    "backpropagation algorithm calculates the gradient of the error function\n",
    "for a single training example, which needs to be generalized to the\n",
    "overall error function. The second assumption is that it can be written\n",
    "as a function of the outputs from the neural network.\n",
    "\n",
    "### Example loss function\n",
    "\n",
    "Let $y,y'$ be vectors in $\\mathbb{R}^n$.\n",
    "\n",
    "Select an error function $E(y,y')$ measuring the difference between two\n",
    "outputs.\n",
    "\n",
    "The standard choice is $E(y,y') = \\tfrac{1}{2} \\lVert y-y'\\rVert^2$,\n",
    "\n",
    "the square of the Euclidean distance between the vectors $y$ and $y'$.\n",
    "\n",
    "The factor of $\\tfrac{1}{2}$ conveniently cancels the exponent when the\n",
    "error function is subsequently differentiated.\n",
    "\n",
    "The error function over $n$ training examples can be written as an\n",
    "average$$E=\\frac{1}{2n}\\sum_x\\lVert (y(x)-y'(x)) \\rVert^2$$And the\n",
    "partial derivative with respect to the\n",
    "outputs$$\\frac{\\partial E}{\\partial y'} = y'-y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "**Backpropagation** is a method used in artificial neural networks to\n",
    "calculate a gradient that is needed in the calculation of the weights to\n",
    "be used in the network. It is commonly used to train deep neural\n",
    "networks, a term used to explain neural networks with more than\n",
    "one hidden layer.\n",
    "\n",
    "**What is backpropagation really doing?**\n",
    "\n",
    "![What is backpropagation really doing?](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Backpropagation.png)\n",
    "\n",
    "Slide from [What is backpropagation really doing?](https://youtu.be/Ilg3gGewQ5U) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to MNIST data\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many input neurons?\n",
    "\n",
    "What is the input layer for the MNIST data?  Each image is $28x28 = 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 10K 28×28 sized test images\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multi-layer perceptron model we must reduce the images down into a a single input layer as a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values (28x28=784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are gray scale between 0 and 255. Scaling of input values when using neural network models is a good idea. Neural network models propagate values and the rate of propagation can be effected by their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data between 0 and 1\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network\n",
    "\n",
    "\n",
    "A shallow neural network has few layers (just one dense layer in this case). Dense means every neuron connected to every other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.72549021,  0.62352943,\n",
       "        0.59215689,  0.23529412,  0.14117648,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.87058824,  0.99607843,  0.99607843,  0.99607843,  0.99607843,\n",
       "        0.94509804,  0.7764706 ,  0.7764706 ,  0.7764706 ,  0.7764706 ,\n",
       "        0.7764706 ,  0.7764706 ,  0.7764706 ,  0.7764706 ,  0.66666669,\n",
       "        0.20392157,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.26274511,  0.44705883,\n",
       "        0.28235295,  0.44705883,  0.63921571,  0.89019608,  0.99607843,\n",
       "        0.88235295,  0.99607843,  0.99607843,  0.99607843,  0.98039216,\n",
       "        0.89803922,  0.99607843,  0.99607843,  0.54901963,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.06666667,  0.25882354,  0.05490196,  0.26274511,\n",
       "        0.26274511,  0.26274511,  0.23137255,  0.08235294,  0.9254902 ,\n",
       "        0.99607843,  0.41568628,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32549021,  0.99215686,  0.81960785,  0.07058824,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.08627451,  0.9137255 ,\n",
       "        1.        ,  0.32549021,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.50588238,  0.99607843,  0.93333334,  0.17254902,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.23137255,  0.97647059,\n",
       "        0.99607843,  0.24313726,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.52156866,  0.99607843,  0.73333335,  0.01960784,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03529412,  0.80392158,\n",
       "        0.97254902,  0.22745098,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.49411765,  0.99607843,  0.71372551,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.29411766,  0.98431373,\n",
       "        0.94117647,  0.22352941,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.07450981,  0.86666667,  0.99607843,  0.65098041,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01176471,  0.79607844,  0.99607843,\n",
       "        0.85882354,  0.13725491,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.99607843,  0.99607843,  0.3019608 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.12156863,  0.87843138,  0.99607843,\n",
       "        0.4509804 ,  0.00392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.52156866,  0.99607843,  0.99607843,  0.20392157,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.23921569,  0.94901961,  0.99607843,\n",
       "        0.99607843,  0.20392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.47450981,  0.99607843,  0.99607843,  0.85882354,  0.15686275,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.47450981,  0.99607843,\n",
       "        0.81176472,  0.07058824,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_A(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='sigmoid', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=shallow_net_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0917 - acc: 0.1311 - val_loss: 0.0912 - val_acc: 0.1584\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0909 - acc: 0.1665 - val_loss: 0.0906 - val_acc: 0.1890\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0903 - acc: 0.1921 - val_loss: 0.0900 - val_acc: 0.2061\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0898 - acc: 0.2053 - val_loss: 0.0895 - val_acc: 0.2177\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0893 - acc: 0.2143 - val_loss: 0.0891 - val_acc: 0.2236\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0889 - acc: 0.2214 - val_loss: 0.0887 - val_acc: 0.2319\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0886 - acc: 0.2307 - val_loss: 0.0884 - val_acc: 0.2399\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0882 - acc: 0.2437 - val_loss: 0.0880 - val_acc: 0.2622\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0879 - acc: 0.2816 - val_loss: 0.0877 - val_acc: 0.3137\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0876 - acc: 0.3241 - val_loss: 0.0874 - val_acc: 0.3494\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0873 - acc: 0.3484 - val_loss: 0.0870 - val_acc: 0.3649\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0870 - acc: 0.3626 - val_loss: 0.0867 - val_acc: 0.3727\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0867 - acc: 0.3748 - val_loss: 0.0864 - val_acc: 0.3799\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0864 - acc: 0.3842 - val_loss: 0.0861 - val_acc: 0.3860\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0860 - acc: 0.3902 - val_loss: 0.0857 - val_acc: 0.3922\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0857 - acc: 0.3950 - val_loss: 0.0854 - val_acc: 0.3967\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0854 - acc: 0.4001 - val_loss: 0.0851 - val_acc: 0.4014\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0851 - acc: 0.4044 - val_loss: 0.0847 - val_acc: 0.4052\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0847 - acc: 0.4066 - val_loss: 0.0844 - val_acc: 0.4084\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0844 - acc: 0.4089 - val_loss: 0.0840 - val_acc: 0.4139\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0840 - acc: 0.4134 - val_loss: 0.0837 - val_acc: 0.4157\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0837 - acc: 0.4157 - val_loss: 0.0833 - val_acc: 0.4201\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0833 - acc: 0.4198 - val_loss: 0.0829 - val_acc: 0.4226\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0829 - acc: 0.4236 - val_loss: 0.0825 - val_acc: 0.4261\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0825 - acc: 0.4282 - val_loss: 0.0821 - val_acc: 0.4286\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0821 - acc: 0.4309 - val_loss: 0.0817 - val_acc: 0.4330\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0817 - acc: 0.4344 - val_loss: 0.0813 - val_acc: 0.4386\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0813 - acc: 0.4378 - val_loss: 0.0809 - val_acc: 0.4438\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0809 - acc: 0.4431 - val_loss: 0.0805 - val_acc: 0.4490\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0805 - acc: 0.4473 - val_loss: 0.0800 - val_acc: 0.4535\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0801 - acc: 0.4525 - val_loss: 0.0796 - val_acc: 0.4575\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0796 - acc: 0.4566 - val_loss: 0.0791 - val_acc: 0.4615\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0792 - acc: 0.4617 - val_loss: 0.0787 - val_acc: 0.4668\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0787 - acc: 0.4665 - val_loss: 0.0782 - val_acc: 0.4715\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0783 - acc: 0.4722 - val_loss: 0.0778 - val_acc: 0.4776\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0778 - acc: 0.4772 - val_loss: 0.0773 - val_acc: 0.4841\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0774 - acc: 0.4824 - val_loss: 0.0768 - val_acc: 0.4886\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0769 - acc: 0.4884 - val_loss: 0.0764 - val_acc: 0.4951\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0764 - acc: 0.4937 - val_loss: 0.0759 - val_acc: 0.4994\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0760 - acc: 0.4997 - val_loss: 0.0754 - val_acc: 0.5050\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0755 - acc: 0.5049 - val_loss: 0.0749 - val_acc: 0.5098\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0750 - acc: 0.5105 - val_loss: 0.0745 - val_acc: 0.5144\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0746 - acc: 0.5161 - val_loss: 0.0740 - val_acc: 0.5208\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0741 - acc: 0.5211 - val_loss: 0.0735 - val_acc: 0.5253\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0736 - acc: 0.5267 - val_loss: 0.0730 - val_acc: 0.5302\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0731 - acc: 0.5316 - val_loss: 0.0725 - val_acc: 0.5350\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0726 - acc: 0.5363 - val_loss: 0.0720 - val_acc: 0.5410\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0722 - acc: 0.5414 - val_loss: 0.0716 - val_acc: 0.5462\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0717 - acc: 0.5458 - val_loss: 0.0711 - val_acc: 0.5505\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0712 - acc: 0.5495 - val_loss: 0.0706 - val_acc: 0.5552\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0707 - acc: 0.5535 - val_loss: 0.0701 - val_acc: 0.5599\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0702 - acc: 0.5576 - val_loss: 0.0696 - val_acc: 0.5646\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0698 - acc: 0.5616 - val_loss: 0.0691 - val_acc: 0.5686\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0693 - acc: 0.5645 - val_loss: 0.0686 - val_acc: 0.5718\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0688 - acc: 0.5689 - val_loss: 0.0682 - val_acc: 0.5740\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0683 - acc: 0.5719 - val_loss: 0.0677 - val_acc: 0.5779\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0678 - acc: 0.5767 - val_loss: 0.0672 - val_acc: 0.5810\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0674 - acc: 0.5795 - val_loss: 0.0667 - val_acc: 0.5834\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0669 - acc: 0.5828 - val_loss: 0.0662 - val_acc: 0.5856\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0664 - acc: 0.5855 - val_loss: 0.0657 - val_acc: 0.5882\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0659 - acc: 0.5886 - val_loss: 0.0653 - val_acc: 0.5908\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0655 - acc: 0.5914 - val_loss: 0.0648 - val_acc: 0.5946\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0650 - acc: 0.5942 - val_loss: 0.0643 - val_acc: 0.5982\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0645 - acc: 0.5970 - val_loss: 0.0638 - val_acc: 0.6004\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0641 - acc: 0.5994 - val_loss: 0.0634 - val_acc: 0.6026\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0636 - acc: 0.6021 - val_loss: 0.0629 - val_acc: 0.6047\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0631 - acc: 0.6043 - val_loss: 0.0624 - val_acc: 0.6078\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0627 - acc: 0.6066 - val_loss: 0.0620 - val_acc: 0.6101\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0622 - acc: 0.6088 - val_loss: 0.0615 - val_acc: 0.6130\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0618 - acc: 0.6111 - val_loss: 0.0610 - val_acc: 0.6145\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0613 - acc: 0.6139 - val_loss: 0.0606 - val_acc: 0.6177\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0609 - acc: 0.6165 - val_loss: 0.0601 - val_acc: 0.6197\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0604 - acc: 0.6193 - val_loss: 0.0597 - val_acc: 0.6215\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0600 - acc: 0.6221 - val_loss: 0.0592 - val_acc: 0.6244\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0595 - acc: 0.6250 - val_loss: 0.0588 - val_acc: 0.6265\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0591 - acc: 0.6274 - val_loss: 0.0584 - val_acc: 0.6291\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0587 - acc: 0.6302 - val_loss: 0.0579 - val_acc: 0.6334\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0583 - acc: 0.6329 - val_loss: 0.0575 - val_acc: 0.6360\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0578 - acc: 0.6356 - val_loss: 0.0571 - val_acc: 0.6387\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0574 - acc: 0.6386 - val_loss: 0.0567 - val_acc: 0.6432\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0570 - acc: 0.6421 - val_loss: 0.0562 - val_acc: 0.6467\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0566 - acc: 0.6453 - val_loss: 0.0558 - val_acc: 0.6507\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0562 - acc: 0.6491 - val_loss: 0.0554 - val_acc: 0.6538\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0558 - acc: 0.6526 - val_loss: 0.0550 - val_acc: 0.6565\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0554 - acc: 0.6556 - val_loss: 0.0546 - val_acc: 0.6606\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0550 - acc: 0.6591 - val_loss: 0.0542 - val_acc: 0.6640\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0546 - acc: 0.6623 - val_loss: 0.0538 - val_acc: 0.6682\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0542 - acc: 0.6659 - val_loss: 0.0534 - val_acc: 0.6715\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0539 - acc: 0.6690 - val_loss: 0.0531 - val_acc: 0.6757\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0535 - acc: 0.6721 - val_loss: 0.0527 - val_acc: 0.6791\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0531 - acc: 0.6753 - val_loss: 0.0523 - val_acc: 0.6824\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0527 - acc: 0.6787 - val_loss: 0.0520 - val_acc: 0.6855\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0524 - acc: 0.6823 - val_loss: 0.0516 - val_acc: 0.6894\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0520 - acc: 0.6857 - val_loss: 0.0512 - val_acc: 0.6930\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0517 - acc: 0.6889 - val_loss: 0.0509 - val_acc: 0.6972\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0513 - acc: 0.6920 - val_loss: 0.0505 - val_acc: 0.7001\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0510 - acc: 0.6962 - val_loss: 0.0502 - val_acc: 0.7031\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0507 - acc: 0.6994 - val_loss: 0.0499 - val_acc: 0.7071\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0503 - acc: 0.7025 - val_loss: 0.0495 - val_acc: 0.7110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132f0dd30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.049521747672557834, 0.71099999999999997]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 68% accuracy after 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0500 - acc: 0.7057 - val_loss: 0.0492 - val_acc: 0.7145\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0497 - acc: 0.7088 - val_loss: 0.0489 - val_acc: 0.7168\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0494 - acc: 0.7120 - val_loss: 0.0485 - val_acc: 0.7199\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0491 - acc: 0.7148 - val_loss: 0.0482 - val_acc: 0.7226\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0487 - acc: 0.7175 - val_loss: 0.0479 - val_acc: 0.7252\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0484 - acc: 0.7200 - val_loss: 0.0476 - val_acc: 0.7280\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0481 - acc: 0.7222 - val_loss: 0.0473 - val_acc: 0.7313\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0478 - acc: 0.7245 - val_loss: 0.0470 - val_acc: 0.7341\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0476 - acc: 0.7272 - val_loss: 0.0467 - val_acc: 0.7366\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0473 - acc: 0.7292 - val_loss: 0.0464 - val_acc: 0.7389\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0470 - acc: 0.7316 - val_loss: 0.0461 - val_acc: 0.7408\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0467 - acc: 0.7341 - val_loss: 0.0459 - val_acc: 0.7431\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0464 - acc: 0.7364 - val_loss: 0.0456 - val_acc: 0.7457\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0461 - acc: 0.7383 - val_loss: 0.0453 - val_acc: 0.7479\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0459 - acc: 0.7407 - val_loss: 0.0450 - val_acc: 0.7491\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0456 - acc: 0.7425 - val_loss: 0.0448 - val_acc: 0.7507\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0453 - acc: 0.7447 - val_loss: 0.0445 - val_acc: 0.7530\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0451 - acc: 0.7466 - val_loss: 0.0442 - val_acc: 0.7545\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0448 - acc: 0.7483 - val_loss: 0.0440 - val_acc: 0.7561\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0446 - acc: 0.7501 - val_loss: 0.0437 - val_acc: 0.7577\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0443 - acc: 0.7516 - val_loss: 0.0435 - val_acc: 0.7602\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0441 - acc: 0.7531 - val_loss: 0.0432 - val_acc: 0.7619\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0438 - acc: 0.7543 - val_loss: 0.0430 - val_acc: 0.7634\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0436 - acc: 0.7558 - val_loss: 0.0427 - val_acc: 0.7645\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0434 - acc: 0.7571 - val_loss: 0.0425 - val_acc: 0.7662\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0431 - acc: 0.7584 - val_loss: 0.0423 - val_acc: 0.7678\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0429 - acc: 0.7596 - val_loss: 0.0420 - val_acc: 0.7692\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0427 - acc: 0.7611 - val_loss: 0.0418 - val_acc: 0.7707\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0424 - acc: 0.7622 - val_loss: 0.0416 - val_acc: 0.7717\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0422 - acc: 0.7633 - val_loss: 0.0413 - val_acc: 0.7727\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0420 - acc: 0.7643 - val_loss: 0.0411 - val_acc: 0.7739\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0418 - acc: 0.7653 - val_loss: 0.0409 - val_acc: 0.7750\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0415 - acc: 0.7662 - val_loss: 0.0407 - val_acc: 0.7760\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0413 - acc: 0.7673 - val_loss: 0.0405 - val_acc: 0.7769\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0411 - acc: 0.7687 - val_loss: 0.0402 - val_acc: 0.7781\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0409 - acc: 0.7691 - val_loss: 0.0400 - val_acc: 0.7792\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0407 - acc: 0.7703 - val_loss: 0.0398 - val_acc: 0.7799\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0405 - acc: 0.7714 - val_loss: 0.0396 - val_acc: 0.7807\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0403 - acc: 0.7720 - val_loss: 0.0394 - val_acc: 0.7818\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0401 - acc: 0.7730 - val_loss: 0.0392 - val_acc: 0.7821\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0399 - acc: 0.7738 - val_loss: 0.0390 - val_acc: 0.7831\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0397 - acc: 0.7744 - val_loss: 0.0388 - val_acc: 0.7840\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0395 - acc: 0.7751 - val_loss: 0.0386 - val_acc: 0.7847\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0393 - acc: 0.7762 - val_loss: 0.0384 - val_acc: 0.7854\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0391 - acc: 0.7770 - val_loss: 0.0382 - val_acc: 0.7861\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0389 - acc: 0.7777 - val_loss: 0.0380 - val_acc: 0.7872\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0387 - acc: 0.7783 - val_loss: 0.0378 - val_acc: 0.7876\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0385 - acc: 0.7790 - val_loss: 0.0376 - val_acc: 0.7884\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0383 - acc: 0.7797 - val_loss: 0.0374 - val_acc: 0.7892\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0382 - acc: 0.7805 - val_loss: 0.0372 - val_acc: 0.7900\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0380 - acc: 0.7814 - val_loss: 0.0371 - val_acc: 0.7905\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0378 - acc: 0.7823 - val_loss: 0.0369 - val_acc: 0.7919\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0376 - acc: 0.7832 - val_loss: 0.0367 - val_acc: 0.7926\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0374 - acc: 0.7842 - val_loss: 0.0365 - val_acc: 0.7929\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0372 - acc: 0.7851 - val_loss: 0.0363 - val_acc: 0.7941\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0371 - acc: 0.7861 - val_loss: 0.0361 - val_acc: 0.7949\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0369 - acc: 0.7874 - val_loss: 0.0360 - val_acc: 0.7967\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0367 - acc: 0.7887 - val_loss: 0.0358 - val_acc: 0.7978\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0365 - acc: 0.7898 - val_loss: 0.0356 - val_acc: 0.7991\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0364 - acc: 0.7913 - val_loss: 0.0354 - val_acc: 0.8008\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0362 - acc: 0.7926 - val_loss: 0.0352 - val_acc: 0.8023\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0360 - acc: 0.7943 - val_loss: 0.0351 - val_acc: 0.8042\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0358 - acc: 0.7962 - val_loss: 0.0349 - val_acc: 0.8062\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0357 - acc: 0.7976 - val_loss: 0.0347 - val_acc: 0.8077\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0355 - acc: 0.7993 - val_loss: 0.0345 - val_acc: 0.8099\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0353 - acc: 0.8009 - val_loss: 0.0344 - val_acc: 0.8108\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0352 - acc: 0.8029 - val_loss: 0.0342 - val_acc: 0.8120\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0350 - acc: 0.8047 - val_loss: 0.0340 - val_acc: 0.8130\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0348 - acc: 0.8066 - val_loss: 0.0339 - val_acc: 0.8143\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0347 - acc: 0.8081 - val_loss: 0.0337 - val_acc: 0.8161\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0345 - acc: 0.8099 - val_loss: 0.0335 - val_acc: 0.8178\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0343 - acc: 0.8117 - val_loss: 0.0334 - val_acc: 0.8203\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0342 - acc: 0.8133 - val_loss: 0.0332 - val_acc: 0.8226\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0340 - acc: 0.8150 - val_loss: 0.0330 - val_acc: 0.8241\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0338 - acc: 0.8163 - val_loss: 0.0329 - val_acc: 0.8258\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0337 - acc: 0.8182 - val_loss: 0.0327 - val_acc: 0.8278\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0335 - acc: 0.8198 - val_loss: 0.0326 - val_acc: 0.8290\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0334 - acc: 0.8213 - val_loss: 0.0324 - val_acc: 0.8299\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0332 - acc: 0.8227 - val_loss: 0.0322 - val_acc: 0.8318\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0331 - acc: 0.8243 - val_loss: 0.0321 - val_acc: 0.8333\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0329 - acc: 0.8255 - val_loss: 0.0319 - val_acc: 0.8352\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0328 - acc: 0.8267 - val_loss: 0.0318 - val_acc: 0.8372\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0326 - acc: 0.8280 - val_loss: 0.0316 - val_acc: 0.8388\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0325 - acc: 0.8296 - val_loss: 0.0315 - val_acc: 0.8393\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0323 - acc: 0.8307 - val_loss: 0.0313 - val_acc: 0.8404\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0322 - acc: 0.8322 - val_loss: 0.0312 - val_acc: 0.8417\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0320 - acc: 0.8336 - val_loss: 0.0310 - val_acc: 0.8429\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0319 - acc: 0.8347 - val_loss: 0.0309 - val_acc: 0.8434\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0317 - acc: 0.8357 - val_loss: 0.0307 - val_acc: 0.8447\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0316 - acc: 0.8365 - val_loss: 0.0306 - val_acc: 0.8457\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0315 - acc: 0.8376 - val_loss: 0.0305 - val_acc: 0.8468\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0313 - acc: 0.8385 - val_loss: 0.0303 - val_acc: 0.8473\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0312 - acc: 0.8397 - val_loss: 0.0302 - val_acc: 0.8478\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0310 - acc: 0.8405 - val_loss: 0.0300 - val_acc: 0.8488\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0309 - acc: 0.8413 - val_loss: 0.0299 - val_acc: 0.8494\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0308 - acc: 0.8423 - val_loss: 0.0298 - val_acc: 0.8502\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0306 - acc: 0.8429 - val_loss: 0.0296 - val_acc: 0.8514\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0305 - acc: 0.8440 - val_loss: 0.0295 - val_acc: 0.8525\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0304 - acc: 0.8447 - val_loss: 0.0294 - val_acc: 0.8528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f805400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7648/10000 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.029366674435138702, 0.8528]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86% accuracy after another 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0302 - acc: 0.8455 - val_loss: 0.0292 - val_acc: 0.8535\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0301 - acc: 0.8463 - val_loss: 0.0291 - val_acc: 0.8541\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0300 - acc: 0.8469 - val_loss: 0.0290 - val_acc: 0.8547\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0299 - acc: 0.8474 - val_loss: 0.0289 - val_acc: 0.8552\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0297 - acc: 0.8481 - val_loss: 0.0287 - val_acc: 0.8562\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0296 - acc: 0.8488 - val_loss: 0.0286 - val_acc: 0.8570\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0295 - acc: 0.8495 - val_loss: 0.0285 - val_acc: 0.8580\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0294 - acc: 0.8500 - val_loss: 0.0284 - val_acc: 0.8593\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0292 - acc: 0.8506 - val_loss: 0.0282 - val_acc: 0.8597\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0291 - acc: 0.8514 - val_loss: 0.0281 - val_acc: 0.8600\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0290 - acc: 0.8520 - val_loss: 0.0280 - val_acc: 0.8602\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0289 - acc: 0.8523 - val_loss: 0.0279 - val_acc: 0.8613\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0288 - acc: 0.8529 - val_loss: 0.0278 - val_acc: 0.8614\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0287 - acc: 0.8535 - val_loss: 0.0277 - val_acc: 0.8620\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0285 - acc: 0.8540 - val_loss: 0.0275 - val_acc: 0.8622\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0284 - acc: 0.8543 - val_loss: 0.0274 - val_acc: 0.8626\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0283 - acc: 0.8548 - val_loss: 0.0273 - val_acc: 0.8631\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0282 - acc: 0.8552 - val_loss: 0.0272 - val_acc: 0.8634\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0281 - acc: 0.8559 - val_loss: 0.0271 - val_acc: 0.8636\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0280 - acc: 0.8563 - val_loss: 0.0270 - val_acc: 0.8641\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0279 - acc: 0.8569 - val_loss: 0.0269 - val_acc: 0.8644\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0278 - acc: 0.8575 - val_loss: 0.0268 - val_acc: 0.8649\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0277 - acc: 0.8580 - val_loss: 0.0267 - val_acc: 0.8651\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0276 - acc: 0.8584 - val_loss: 0.0266 - val_acc: 0.8652\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0275 - acc: 0.8589 - val_loss: 0.0265 - val_acc: 0.8653\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0274 - acc: 0.8593 - val_loss: 0.0264 - val_acc: 0.8660\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0273 - acc: 0.8597 - val_loss: 0.0263 - val_acc: 0.8663\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0272 - acc: 0.8600 - val_loss: 0.0262 - val_acc: 0.8667\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0271 - acc: 0.8603 - val_loss: 0.0261 - val_acc: 0.8673\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0270 - acc: 0.8609 - val_loss: 0.0260 - val_acc: 0.8679\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0269 - acc: 0.8613 - val_loss: 0.0259 - val_acc: 0.8682\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0268 - acc: 0.8617 - val_loss: 0.0258 - val_acc: 0.8687\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0267 - acc: 0.8622 - val_loss: 0.0257 - val_acc: 0.8688\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0266 - acc: 0.8626 - val_loss: 0.0256 - val_acc: 0.8694\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0265 - acc: 0.8628 - val_loss: 0.0255 - val_acc: 0.8701\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0264 - acc: 0.8631 - val_loss: 0.0254 - val_acc: 0.8710\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0263 - acc: 0.8634 - val_loss: 0.0254 - val_acc: 0.8713\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0263 - acc: 0.8636 - val_loss: 0.0253 - val_acc: 0.8714\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0262 - acc: 0.8641 - val_loss: 0.0252 - val_acc: 0.8716\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0261 - acc: 0.8645 - val_loss: 0.0251 - val_acc: 0.8722\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0260 - acc: 0.8647 - val_loss: 0.0250 - val_acc: 0.8723\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0259 - acc: 0.8649 - val_loss: 0.0249 - val_acc: 0.8725\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0258 - acc: 0.8654 - val_loss: 0.0248 - val_acc: 0.8729\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0257 - acc: 0.8656 - val_loss: 0.0248 - val_acc: 0.8732\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0257 - acc: 0.8660 - val_loss: 0.0247 - val_acc: 0.8733\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0256 - acc: 0.8664 - val_loss: 0.0246 - val_acc: 0.8738\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0255 - acc: 0.8665 - val_loss: 0.0245 - val_acc: 0.8740\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0254 - acc: 0.8669 - val_loss: 0.0244 - val_acc: 0.8744\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0253 - acc: 0.8674 - val_loss: 0.0244 - val_acc: 0.8746\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0253 - acc: 0.8676 - val_loss: 0.0243 - val_acc: 0.8748\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0252 - acc: 0.8679 - val_loss: 0.0242 - val_acc: 0.8751\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0251 - acc: 0.8682 - val_loss: 0.0241 - val_acc: 0.8754\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0250 - acc: 0.8686 - val_loss: 0.0241 - val_acc: 0.8761\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0250 - acc: 0.8689 - val_loss: 0.0240 - val_acc: 0.8762\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0249 - acc: 0.8694 - val_loss: 0.0239 - val_acc: 0.8764\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0248 - acc: 0.8697 - val_loss: 0.0238 - val_acc: 0.8766\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0247 - acc: 0.8699 - val_loss: 0.0238 - val_acc: 0.8768\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0247 - acc: 0.8701 - val_loss: 0.0237 - val_acc: 0.8777\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0246 - acc: 0.8704 - val_loss: 0.0236 - val_acc: 0.8783\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0245 - acc: 0.8705 - val_loss: 0.0236 - val_acc: 0.8790\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0245 - acc: 0.8708 - val_loss: 0.0235 - val_acc: 0.8789\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0244 - acc: 0.8711 - val_loss: 0.0234 - val_acc: 0.8790\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0243 - acc: 0.8714 - val_loss: 0.0233 - val_acc: 0.8795\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0243 - acc: 0.8716 - val_loss: 0.0233 - val_acc: 0.8797\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0242 - acc: 0.8719 - val_loss: 0.0232 - val_acc: 0.8801\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0241 - acc: 0.8722 - val_loss: 0.0231 - val_acc: 0.8805\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0241 - acc: 0.8725 - val_loss: 0.0231 - val_acc: 0.8805\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0240 - acc: 0.8726 - val_loss: 0.0230 - val_acc: 0.8804\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0239 - acc: 0.8730 - val_loss: 0.0230 - val_acc: 0.8804\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0239 - acc: 0.8730 - val_loss: 0.0229 - val_acc: 0.8807\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0238 - acc: 0.8735 - val_loss: 0.0228 - val_acc: 0.8811\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0237 - acc: 0.8738 - val_loss: 0.0228 - val_acc: 0.8815\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0237 - acc: 0.8740 - val_loss: 0.0227 - val_acc: 0.8819\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0236 - acc: 0.8743 - val_loss: 0.0226 - val_acc: 0.8818\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0236 - acc: 0.8742 - val_loss: 0.0226 - val_acc: 0.8822\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0235 - acc: 0.8745 - val_loss: 0.0225 - val_acc: 0.8823\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0234 - acc: 0.8748 - val_loss: 0.0225 - val_acc: 0.8825\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0234 - acc: 0.8750 - val_loss: 0.0224 - val_acc: 0.8826\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0233 - acc: 0.8754 - val_loss: 0.0223 - val_acc: 0.8826\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0233 - acc: 0.8757 - val_loss: 0.0223 - val_acc: 0.8828\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0232 - acc: 0.8759 - val_loss: 0.0222 - val_acc: 0.8828\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0231 - acc: 0.8762 - val_loss: 0.0222 - val_acc: 0.8829\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0231 - acc: 0.8763 - val_loss: 0.0221 - val_acc: 0.8831\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0230 - acc: 0.8765 - val_loss: 0.0221 - val_acc: 0.8832\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0230 - acc: 0.8766 - val_loss: 0.0220 - val_acc: 0.8836\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0229 - acc: 0.8767 - val_loss: 0.0220 - val_acc: 0.8836\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0229 - acc: 0.8768 - val_loss: 0.0219 - val_acc: 0.8840\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0228 - acc: 0.8770 - val_loss: 0.0219 - val_acc: 0.8841\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0228 - acc: 0.8773 - val_loss: 0.0218 - val_acc: 0.8843\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0227 - acc: 0.8776 - val_loss: 0.0217 - val_acc: 0.8847\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0227 - acc: 0.8776 - val_loss: 0.0217 - val_acc: 0.8846\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0226 - acc: 0.8779 - val_loss: 0.0216 - val_acc: 0.8848\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0226 - acc: 0.8780 - val_loss: 0.0216 - val_acc: 0.8848\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0225 - acc: 0.8782 - val_loss: 0.0215 - val_acc: 0.8850\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0225 - acc: 0.8784 - val_loss: 0.0215 - val_acc: 0.8851\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0224 - acc: 0.8787 - val_loss: 0.0214 - val_acc: 0.8856\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0224 - acc: 0.8789 - val_loss: 0.0214 - val_acc: 0.8854\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0223 - acc: 0.8791 - val_loss: 0.0213 - val_acc: 0.8855\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0223 - acc: 0.8792 - val_loss: 0.0213 - val_acc: 0.8857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132d14f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9760/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.021299916153401138, 0.88570000000000004]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 88% accuracy after another 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of activation functions\n",
    "\n",
    "Some desirable properties in an activation function include:\n",
    "\n",
    "-   Nonlinear – When the activation function is non-linear, then a\n",
    "    two-layer neural network can be proven to be a universal function\n",
    "    approximator. The identity activation function does not satisfy\n",
    "    this property. When multiple layers use the identity activation\n",
    "    function, the entire network is equivalent to a single-layer model.   \n",
    "-   Continuously differentiable – This property is necessary for\n",
    "    enabling gradient-based optimization methods. The binary step\n",
    "    activation function is not differentiable at 0, and it\n",
    "    differentiates to 0 for all other values, so gradient-based methods\n",
    "    can make no progress with it.   \n",
    "-   Range – When the range of the activation function is finite,\n",
    "    gradient-based training methods tend to be more stable, because\n",
    "    pattern presentations significantly affect only limited weights.\n",
    "    When the range is infinite, training is generally more efficient\n",
    "    because pattern presentations significantly affect most of the\n",
    "    weights. In the latter case, smaller learning rates are typically\n",
    "    necessary.   \n",
    "-   Monotonic – When the activation function is monotonic, the error\n",
    "    surface associated with a single-layer model is guaranteed to be\n",
    "    convex.   \n",
    "-   Smooth Functions with a Monotonic derivative – These have been shown\n",
    "    to generalize better in some cases. The argument for these\n",
    "    properties suggests that such activation functions are more\n",
    "    consistent with Occam's razor.   \n",
    "-   Approximates identity near the origin – When activation functions\n",
    "    have this property, the neural network will learn efficiently when\n",
    "    its weights are initialized with small random values. When the\n",
    "    activation function does not approximate identity near the origin,\n",
    "    special care must be used when initializing the weights.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_B(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    # Using relu\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn2=shallow_net_B()\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0877 - acc: 0.1950 - val_loss: 0.0861 - val_acc: 0.2297\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0850 - acc: 0.2565 - val_loss: 0.0833 - val_acc: 0.2926\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0821 - acc: 0.3206 - val_loss: 0.0803 - val_acc: 0.3649\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0793 - acc: 0.3886 - val_loss: 0.0774 - val_acc: 0.4283\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0764 - acc: 0.4416 - val_loss: 0.0744 - val_acc: 0.4761\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0735 - acc: 0.4862 - val_loss: 0.0713 - val_acc: 0.5206\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0705 - acc: 0.5296 - val_loss: 0.0681 - val_acc: 0.5606\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0673 - acc: 0.5721 - val_loss: 0.0647 - val_acc: 0.5992\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0640 - acc: 0.6111 - val_loss: 0.0612 - val_acc: 0.6430\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0605 - acc: 0.6469 - val_loss: 0.0576 - val_acc: 0.6781\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0570 - acc: 0.6789 - val_loss: 0.0541 - val_acc: 0.7078\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0537 - acc: 0.7053 - val_loss: 0.0508 - val_acc: 0.7322\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0506 - acc: 0.7285 - val_loss: 0.0478 - val_acc: 0.7559\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0477 - acc: 0.7489 - val_loss: 0.0451 - val_acc: 0.7734\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0452 - acc: 0.7668 - val_loss: 0.0426 - val_acc: 0.7905\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0428 - acc: 0.7834 - val_loss: 0.0404 - val_acc: 0.8033\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0407 - acc: 0.7964 - val_loss: 0.0384 - val_acc: 0.8142\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0389 - acc: 0.8071 - val_loss: 0.0366 - val_acc: 0.8239\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0372 - acc: 0.8155 - val_loss: 0.0350 - val_acc: 0.8306\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0357 - acc: 0.8221 - val_loss: 0.0336 - val_acc: 0.8353\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0343 - acc: 0.8272 - val_loss: 0.0324 - val_acc: 0.8411\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0331 - acc: 0.8318 - val_loss: 0.0312 - val_acc: 0.8451\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0320 - acc: 0.8353 - val_loss: 0.0302 - val_acc: 0.8479\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0311 - acc: 0.8391 - val_loss: 0.0293 - val_acc: 0.8506\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0302 - acc: 0.8427 - val_loss: 0.0285 - val_acc: 0.8528\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0294 - acc: 0.8455 - val_loss: 0.0277 - val_acc: 0.8555\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0286 - acc: 0.8480 - val_loss: 0.0270 - val_acc: 0.8572\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0280 - acc: 0.8499 - val_loss: 0.0264 - val_acc: 0.8603\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0274 - acc: 0.8520 - val_loss: 0.0258 - val_acc: 0.8619\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0268 - acc: 0.8541 - val_loss: 0.0253 - val_acc: 0.8632\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0263 - acc: 0.8565 - val_loss: 0.0248 - val_acc: 0.8642\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0258 - acc: 0.8583 - val_loss: 0.0243 - val_acc: 0.8670\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0253 - acc: 0.8603 - val_loss: 0.0239 - val_acc: 0.8679\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0249 - acc: 0.8621 - val_loss: 0.0235 - val_acc: 0.8697\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0245 - acc: 0.8640 - val_loss: 0.0231 - val_acc: 0.8712\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0241 - acc: 0.8653 - val_loss: 0.0227 - val_acc: 0.8725\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0238 - acc: 0.8668 - val_loss: 0.0224 - val_acc: 0.8737\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0234 - acc: 0.8682 - val_loss: 0.0221 - val_acc: 0.8752\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0231 - acc: 0.8692 - val_loss: 0.0218 - val_acc: 0.8771\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0228 - acc: 0.8706 - val_loss: 0.0215 - val_acc: 0.8787\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0225 - acc: 0.8716 - val_loss: 0.0212 - val_acc: 0.8804\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0223 - acc: 0.8728 - val_loss: 0.0210 - val_acc: 0.8812\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0220 - acc: 0.8740 - val_loss: 0.0208 - val_acc: 0.8820\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0218 - acc: 0.8747 - val_loss: 0.0205 - val_acc: 0.8824\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0216 - acc: 0.8757 - val_loss: 0.0203 - val_acc: 0.8835\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0213 - acc: 0.8768 - val_loss: 0.0201 - val_acc: 0.8840\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0211 - acc: 0.8775 - val_loss: 0.0199 - val_acc: 0.8849\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0209 - acc: 0.8784 - val_loss: 0.0197 - val_acc: 0.8861\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0207 - acc: 0.8790 - val_loss: 0.0195 - val_acc: 0.8861\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0206 - acc: 0.8797 - val_loss: 0.0194 - val_acc: 0.8866\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0204 - acc: 0.8806 - val_loss: 0.0192 - val_acc: 0.8871\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0202 - acc: 0.8811 - val_loss: 0.0191 - val_acc: 0.8879\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0201 - acc: 0.8816 - val_loss: 0.0189 - val_acc: 0.8892\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0199 - acc: 0.8821 - val_loss: 0.0188 - val_acc: 0.8899\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0198 - acc: 0.8828 - val_loss: 0.0186 - val_acc: 0.8906\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0196 - acc: 0.8835 - val_loss: 0.0185 - val_acc: 0.8915\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0195 - acc: 0.8841 - val_loss: 0.0183 - val_acc: 0.8923\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0193 - acc: 0.8847 - val_loss: 0.0182 - val_acc: 0.8930\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0192 - acc: 0.8852 - val_loss: 0.0181 - val_acc: 0.8932\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0191 - acc: 0.8858 - val_loss: 0.0180 - val_acc: 0.8934\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0190 - acc: 0.8863 - val_loss: 0.0179 - val_acc: 0.8938\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0188 - acc: 0.8869 - val_loss: 0.0177 - val_acc: 0.8942\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0187 - acc: 0.8872 - val_loss: 0.0176 - val_acc: 0.8950\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0186 - acc: 0.8880 - val_loss: 0.0175 - val_acc: 0.8952\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0185 - acc: 0.8884 - val_loss: 0.0174 - val_acc: 0.8959\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0184 - acc: 0.8888 - val_loss: 0.0173 - val_acc: 0.8963\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0183 - acc: 0.8894 - val_loss: 0.0172 - val_acc: 0.8961\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0182 - acc: 0.8898 - val_loss: 0.0171 - val_acc: 0.8965\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0181 - acc: 0.8905 - val_loss: 0.0171 - val_acc: 0.8972\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0180 - acc: 0.8907 - val_loss: 0.0170 - val_acc: 0.8972\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0179 - acc: 0.8912 - val_loss: 0.0169 - val_acc: 0.8982\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0178 - acc: 0.8917 - val_loss: 0.0168 - val_acc: 0.8980\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0177 - acc: 0.8921 - val_loss: 0.0167 - val_acc: 0.8984\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0176 - acc: 0.8926 - val_loss: 0.0166 - val_acc: 0.8991\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0176 - acc: 0.8931 - val_loss: 0.0166 - val_acc: 0.8988\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0175 - acc: 0.8932 - val_loss: 0.0165 - val_acc: 0.8994\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0174 - acc: 0.8937 - val_loss: 0.0164 - val_acc: 0.8999\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0173 - acc: 0.8940 - val_loss: 0.0163 - val_acc: 0.8996\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0172 - acc: 0.8944 - val_loss: 0.0163 - val_acc: 0.9004\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0172 - acc: 0.8946 - val_loss: 0.0162 - val_acc: 0.9009\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0171 - acc: 0.8950 - val_loss: 0.0161 - val_acc: 0.9009\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0170 - acc: 0.8951 - val_loss: 0.0161 - val_acc: 0.9011\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0170 - acc: 0.8955 - val_loss: 0.0160 - val_acc: 0.9016\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0169 - acc: 0.8958 - val_loss: 0.0159 - val_acc: 0.9018\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0168 - acc: 0.8963 - val_loss: 0.0159 - val_acc: 0.9017\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0168 - acc: 0.8965 - val_loss: 0.0158 - val_acc: 0.9021\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0167 - acc: 0.8970 - val_loss: 0.0157 - val_acc: 0.9024\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0166 - acc: 0.8973 - val_loss: 0.0157 - val_acc: 0.9026\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0166 - acc: 0.8976 - val_loss: 0.0156 - val_acc: 0.9028\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0165 - acc: 0.8980 - val_loss: 0.0156 - val_acc: 0.9031\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0164 - acc: 0.8981 - val_loss: 0.0155 - val_acc: 0.9031\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0164 - acc: 0.8985 - val_loss: 0.0155 - val_acc: 0.9033\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0163 - acc: 0.8989 - val_loss: 0.0154 - val_acc: 0.9036\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0163 - acc: 0.8995 - val_loss: 0.0154 - val_acc: 0.9040\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0162 - acc: 0.8999 - val_loss: 0.0153 - val_acc: 0.9037\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0162 - acc: 0.9000 - val_loss: 0.0153 - val_acc: 0.9043\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0161 - acc: 0.9004 - val_loss: 0.0152 - val_acc: 0.9044\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0161 - acc: 0.9007 - val_loss: 0.0152 - val_acc: 0.9044\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0160 - acc: 0.9012 - val_loss: 0.0151 - val_acc: 0.9047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12773b630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8576/10000 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015109153815731406, 0.90469999999999995]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% accuracy after first 99 epochs with Relu\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0160 - acc: 0.9013 - val_loss: 0.0151 - val_acc: 0.9048\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0159 - acc: 0.9015 - val_loss: 0.0150 - val_acc: 0.9051\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0159 - acc: 0.9017 - val_loss: 0.0150 - val_acc: 0.9054\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0158 - acc: 0.9022 - val_loss: 0.0149 - val_acc: 0.9057\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0158 - acc: 0.9024 - val_loss: 0.0149 - val_acc: 0.9058\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0157 - acc: 0.9025 - val_loss: 0.0148 - val_acc: 0.9061\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0157 - acc: 0.9029 - val_loss: 0.0148 - val_acc: 0.9063\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0156 - acc: 0.9031 - val_loss: 0.0148 - val_acc: 0.9065\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0156 - acc: 0.9033 - val_loss: 0.0147 - val_acc: 0.9066\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0155 - acc: 0.9035 - val_loss: 0.0147 - val_acc: 0.9067\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0155 - acc: 0.9038 - val_loss: 0.0146 - val_acc: 0.9067\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0154 - acc: 0.9041 - val_loss: 0.0146 - val_acc: 0.9071\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0154 - acc: 0.9044 - val_loss: 0.0146 - val_acc: 0.9072\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0154 - acc: 0.9046 - val_loss: 0.0145 - val_acc: 0.9080\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0153 - acc: 0.9048 - val_loss: 0.0145 - val_acc: 0.9085\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0153 - acc: 0.9049 - val_loss: 0.0144 - val_acc: 0.9089\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0152 - acc: 0.9052 - val_loss: 0.0144 - val_acc: 0.9093\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0152 - acc: 0.9053 - val_loss: 0.0144 - val_acc: 0.9091\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0151 - acc: 0.9056 - val_loss: 0.0143 - val_acc: 0.9096\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0151 - acc: 0.9058 - val_loss: 0.0143 - val_acc: 0.9101\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0151 - acc: 0.9059 - val_loss: 0.0143 - val_acc: 0.9099\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0150 - acc: 0.9061 - val_loss: 0.0142 - val_acc: 0.9105\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0150 - acc: 0.9063 - val_loss: 0.0142 - val_acc: 0.9108\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0150 - acc: 0.9065 - val_loss: 0.0142 - val_acc: 0.9107\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0149 - acc: 0.9067 - val_loss: 0.0141 - val_acc: 0.9108\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0149 - acc: 0.9070 - val_loss: 0.0141 - val_acc: 0.9112\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0148 - acc: 0.9072 - val_loss: 0.0141 - val_acc: 0.9115\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0148 - acc: 0.9074 - val_loss: 0.0140 - val_acc: 0.9116\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0148 - acc: 0.9076 - val_loss: 0.0140 - val_acc: 0.9120\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0147 - acc: 0.9076 - val_loss: 0.0140 - val_acc: 0.9123\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0147 - acc: 0.9078 - val_loss: 0.0139 - val_acc: 0.9126\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0147 - acc: 0.9080 - val_loss: 0.0139 - val_acc: 0.9129\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0146 - acc: 0.9082 - val_loss: 0.0139 - val_acc: 0.9130\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0146 - acc: 0.9084 - val_loss: 0.0138 - val_acc: 0.9132\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0146 - acc: 0.9087 - val_loss: 0.0138 - val_acc: 0.9137\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0145 - acc: 0.9088 - val_loss: 0.0138 - val_acc: 0.9139\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0145 - acc: 0.9091 - val_loss: 0.0138 - val_acc: 0.9141\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0145 - acc: 0.9092 - val_loss: 0.0137 - val_acc: 0.9142\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9094 - val_loss: 0.0137 - val_acc: 0.9143\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9096 - val_loss: 0.0137 - val_acc: 0.9143\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9099 - val_loss: 0.0136 - val_acc: 0.9142\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9098 - val_loss: 0.0136 - val_acc: 0.9145\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9101 - val_loss: 0.0136 - val_acc: 0.9144\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9102 - val_loss: 0.0136 - val_acc: 0.9146\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9107 - val_loss: 0.0135 - val_acc: 0.9149\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9107 - val_loss: 0.0135 - val_acc: 0.9150\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9108 - val_loss: 0.0135 - val_acc: 0.9151\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9111 - val_loss: 0.0135 - val_acc: 0.9152\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9113 - val_loss: 0.0134 - val_acc: 0.9155\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9113 - val_loss: 0.0134 - val_acc: 0.9159\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9117 - val_loss: 0.0134 - val_acc: 0.9159\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9117 - val_loss: 0.0134 - val_acc: 0.9161\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9120 - val_loss: 0.0133 - val_acc: 0.9163\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9120 - val_loss: 0.0133 - val_acc: 0.9166\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9121 - val_loss: 0.0133 - val_acc: 0.9166\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9123 - val_loss: 0.0133 - val_acc: 0.9167\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9124 - val_loss: 0.0132 - val_acc: 0.9169\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9125 - val_loss: 0.0132 - val_acc: 0.9168\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9127 - val_loss: 0.0132 - val_acc: 0.9169\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9127 - val_loss: 0.0132 - val_acc: 0.9168\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9131 - val_loss: 0.0131 - val_acc: 0.9169\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9131 - val_loss: 0.0131 - val_acc: 0.9171\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9134 - val_loss: 0.0131 - val_acc: 0.9175\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9133 - val_loss: 0.0131 - val_acc: 0.9175\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9137 - val_loss: 0.0131 - val_acc: 0.9176\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9138 - val_loss: 0.0130 - val_acc: 0.9181\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9140 - val_loss: 0.0130 - val_acc: 0.9177\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9141 - val_loss: 0.0130 - val_acc: 0.9178\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9142 - val_loss: 0.0130 - val_acc: 0.9182\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9145 - val_loss: 0.0130 - val_acc: 0.9182\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9144 - val_loss: 0.0129 - val_acc: 0.9183\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9145 - val_loss: 0.0129 - val_acc: 0.9190\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9147 - val_loss: 0.0129 - val_acc: 0.9190\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9148 - val_loss: 0.0129 - val_acc: 0.9192\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9149 - val_loss: 0.0129 - val_acc: 0.9199\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9150 - val_loss: 0.0128 - val_acc: 0.9197\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9152 - val_loss: 0.0128 - val_acc: 0.9201\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9153 - val_loss: 0.0128 - val_acc: 0.9205\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9156 - val_loss: 0.0128 - val_acc: 0.9203\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9157 - val_loss: 0.0128 - val_acc: 0.9203\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9157 - val_loss: 0.0127 - val_acc: 0.9208\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9158 - val_loss: 0.0127 - val_acc: 0.9208\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9161 - val_loss: 0.0127 - val_acc: 0.9210\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9162 - val_loss: 0.0127 - val_acc: 0.9213\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9162 - val_loss: 0.0127 - val_acc: 0.9212\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9164 - val_loss: 0.0126 - val_acc: 0.9217\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9165 - val_loss: 0.0126 - val_acc: 0.9213\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9165 - val_loss: 0.0126 - val_acc: 0.9217\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9167 - val_loss: 0.0126 - val_acc: 0.9216\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9168 - val_loss: 0.0126 - val_acc: 0.9218\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9170 - val_loss: 0.0126 - val_acc: 0.9222\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9172 - val_loss: 0.0125 - val_acc: 0.9223\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9173 - val_loss: 0.0125 - val_acc: 0.9223\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9174 - val_loss: 0.0125 - val_acc: 0.9223\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9177 - val_loss: 0.0125 - val_acc: 0.9224\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0131 - acc: 0.9178 - val_loss: 0.0125 - val_acc: 0.9225\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0130 - acc: 0.9179 - val_loss: 0.0125 - val_acc: 0.9226\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0130 - acc: 0.9180 - val_loss: 0.0124 - val_acc: 0.9229\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9181 - val_loss: 0.0124 - val_acc: 0.9228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132f6e2e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9568/10000 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01241813175752759, 0.92279999999999995]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 92% accuracy after another 99 epochs with Relu\n",
    "# Seems to be a plateau\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of cost functions\n",
    "\n",
    "### Cross entropy\n",
    "\n",
    "In information theory, the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two probability\n",
    "distributions $p$ and $q$ over the same underlying set of events\n",
    "measures the average number of bits needed to identify an event drawn\n",
    "from the set, if a coding scheme is used that is optimized for an\n",
    "“unnatural” probability distribution $q$, rather than the “true”\n",
    "distribution $p$.\n",
    "\n",
    "The cross entropy for the distributions $p$ and $q$ over a given set is\n",
    "defined as follows:\n",
    "\n",
    "$$H(p, q) = \\operatorname{E}_p[-\\log q] = H(p) + D_{\\mathrm{KL}}(p \\| q),\\!$$\n",
    "\n",
    "where $H(p)$ is the entropy of $p$, and $D_{\\mathrm{KL}}(p \\| q)$ is\n",
    "the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) of $q$ from $p$ (also known as the\n",
    "*relative entropy* of *p* with respect to *q* — note the reversal of\n",
    "emphasis).\n",
    "\n",
    "For discrete $p$ and $q$ this means\n",
    "\n",
    "$$H(p, q) = -\\sum_x p(x)\\, \\log q(x). \\!$$\n",
    "\n",
    "The situation for continuous distributions is analogous. We have to\n",
    "assume that $p$ and $q$ are [absolutely continuous] with respect to some\n",
    "reference [measure] $r$ (usually $r$ is a Lebesgue measure on a\n",
    "Borel [σ-algebra]). Let $P$ and $Q$ be probability density functions\n",
    "of $p$ and $q$ with respect to $r$. Then\n",
    "\n",
    "$$-\\int_X P(x)\\, \\log Q(x)\\, dr(x) = \\operatorname{E}_p[-\\log Q]. \\!$$\n",
    "\n",
    "NB: The notation $H(p,q)$ is also used for a different concept, the\n",
    "joint entropy of $p$ and $q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_C(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    # Using relu and \n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn3=shallow_net_C()\n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 1.2356 - acc: 0.6916 - val_loss: 0.6865 - val_acc: 0.8454\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.5838 - acc: 0.8574 - val_loss: 0.4797 - val_acc: 0.8789\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.4597 - acc: 0.8791 - val_loss: 0.4067 - val_acc: 0.8903\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.4057 - acc: 0.8892 - val_loss: 0.3690 - val_acc: 0.8996\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3739 - acc: 0.8957 - val_loss: 0.3453 - val_acc: 0.9055\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3522 - acc: 0.9009 - val_loss: 0.3274 - val_acc: 0.9086\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3358 - acc: 0.9054 - val_loss: 0.3142 - val_acc: 0.9128\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3229 - acc: 0.9087 - val_loss: 0.3031 - val_acc: 0.9146\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.3117 - acc: 0.9112 - val_loss: 0.2940 - val_acc: 0.9177\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.3021 - acc: 0.9138 - val_loss: 0.2869 - val_acc: 0.9192\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2936 - acc: 0.9166 - val_loss: 0.2791 - val_acc: 0.9205\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2860 - acc: 0.9186 - val_loss: 0.2731 - val_acc: 0.9223\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2788 - acc: 0.9208 - val_loss: 0.2663 - val_acc: 0.9239\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2723 - acc: 0.9229 - val_loss: 0.2610 - val_acc: 0.9248\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2662 - acc: 0.9247 - val_loss: 0.2555 - val_acc: 0.9274\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2606 - acc: 0.9265 - val_loss: 0.2509 - val_acc: 0.9283\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2552 - acc: 0.9280 - val_loss: 0.2468 - val_acc: 0.9295\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2502 - acc: 0.9295 - val_loss: 0.2426 - val_acc: 0.9306\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2456 - acc: 0.9308 - val_loss: 0.2379 - val_acc: 0.9325\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2411 - acc: 0.9322 - val_loss: 0.2345 - val_acc: 0.9332\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2367 - acc: 0.933 - 1s - loss: 0.2370 - acc: 0.9334 - val_loss: 0.2303 - val_acc: 0.9328\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2329 - acc: 0.9348 - val_loss: 0.2267 - val_acc: 0.9349\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2291 - acc: 0.9359 - val_loss: 0.2234 - val_acc: 0.9359\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2254 - acc: 0.9369 - val_loss: 0.2203 - val_acc: 0.9366\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2218 - acc: 0.9384 - val_loss: 0.2167 - val_acc: 0.9376\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2185 - acc: 0.9389 - val_loss: 0.2143 - val_acc: 0.9387\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2153 - acc: 0.9398 - val_loss: 0.2112 - val_acc: 0.9387\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2120 - acc: 0.9407 - val_loss: 0.2089 - val_acc: 0.9389\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2090 - acc: 0.9417 - val_loss: 0.2057 - val_acc: 0.9400\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.2060 - acc: 0.9422 - val_loss: 0.2031 - val_acc: 0.9409\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2033 - acc: 0.9430 - val_loss: 0.2013 - val_acc: 0.9406\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2005 - acc: 0.9438 - val_loss: 0.1986 - val_acc: 0.9430\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1978 - acc: 0.9446 - val_loss: 0.1967 - val_acc: 0.9435\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1952 - acc: 0.9456 - val_loss: 0.1940 - val_acc: 0.9441\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1926 - acc: 0.9463 - val_loss: 0.1922 - val_acc: 0.9442\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1902 - acc: 0.9466 - val_loss: 0.1894 - val_acc: 0.9447\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1878 - acc: 0.9476 - val_loss: 0.1874 - val_acc: 0.9455\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1854 - acc: 0.9481 - val_loss: 0.1858 - val_acc: 0.9457\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1832 - acc: 0.9487 - val_loss: 0.1835 - val_acc: 0.9473\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1809 - acc: 0.9495 - val_loss: 0.1824 - val_acc: 0.9458\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1788 - acc: 0.9499 - val_loss: 0.1799 - val_acc: 0.9475\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1768 - acc: 0.9505 - val_loss: 0.1778 - val_acc: 0.9485\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1746 - acc: 0.9511 - val_loss: 0.1762 - val_acc: 0.9481\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1726 - acc: 0.9516 - val_loss: 0.1749 - val_acc: 0.9483\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1706 - acc: 0.9519 - val_loss: 0.1724 - val_acc: 0.9486\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1686 - acc: 0.9529 - val_loss: 0.1707 - val_acc: 0.9490\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1668 - acc: 0.9533 - val_loss: 0.1689 - val_acc: 0.9500\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1651 - acc: 0.9539 - val_loss: 0.1675 - val_acc: 0.9512\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1633 - acc: 0.9543 - val_loss: 0.1660 - val_acc: 0.9513\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1615 - acc: 0.9548 - val_loss: 0.1641 - val_acc: 0.9509\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1598 - acc: 0.9555 - val_loss: 0.1637 - val_acc: 0.9516\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1581 - acc: 0.9557 - val_loss: 0.1613 - val_acc: 0.9524\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1564 - acc: 0.9561 - val_loss: 0.1605 - val_acc: 0.9512\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1548 - acc: 0.9572 - val_loss: 0.1596 - val_acc: 0.9525\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1533 - acc: 0.9573 - val_loss: 0.1576 - val_acc: 0.9541\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1517 - acc: 0.9577 - val_loss: 0.1573 - val_acc: 0.9540\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1502 - acc: 0.9585 - val_loss: 0.1555 - val_acc: 0.9545\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1487 - acc: 0.9587 - val_loss: 0.1537 - val_acc: 0.9552\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1472 - acc: 0.9593 - val_loss: 0.1523 - val_acc: 0.9550\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1457 - acc: 0.9597 - val_loss: 0.1518 - val_acc: 0.9566\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1443 - acc: 0.9599 - val_loss: 0.1508 - val_acc: 0.9563\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1429 - acc: 0.9604 - val_loss: 0.1491 - val_acc: 0.9564\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1416 - acc: 0.9609 - val_loss: 0.1481 - val_acc: 0.9571\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1403 - acc: 0.9614 - val_loss: 0.1466 - val_acc: 0.9576\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1389 - acc: 0.9616 - val_loss: 0.1458 - val_acc: 0.9578\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1376 - acc: 0.9621 - val_loss: 0.1441 - val_acc: 0.9577\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1363 - acc: 0.9623 - val_loss: 0.1441 - val_acc: 0.9578\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1352 - acc: 0.9624 - val_loss: 0.1427 - val_acc: 0.9585\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1339 - acc: 0.9626 - val_loss: 0.1417 - val_acc: 0.9588\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1327 - acc: 0.9631 - val_loss: 0.1413 - val_acc: 0.9593\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1315 - acc: 0.9636 - val_loss: 0.1400 - val_acc: 0.9596\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1303 - acc: 0.9638 - val_loss: 0.1392 - val_acc: 0.9602\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1292 - acc: 0.9643 - val_loss: 0.1383 - val_acc: 0.9596\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1281 - acc: 0.9647 - val_loss: 0.1374 - val_acc: 0.9602\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1270 - acc: 0.9652 - val_loss: 0.1365 - val_acc: 0.9609\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1259 - acc: 0.9652 - val_loss: 0.1355 - val_acc: 0.9612\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1248 - acc: 0.9657 - val_loss: 0.1352 - val_acc: 0.9612\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1239 - acc: 0.9655 - val_loss: 0.1336 - val_acc: 0.9611\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1228 - acc: 0.9664 - val_loss: 0.1333 - val_acc: 0.9615\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1219 - acc: 0.9663 - val_loss: 0.1322 - val_acc: 0.9617\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1209 - acc: 0.9667 - val_loss: 0.1314 - val_acc: 0.9615\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1199 - acc: 0.9672 - val_loss: 0.1311 - val_acc: 0.9623\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1190 - acc: 0.9673 - val_loss: 0.1303 - val_acc: 0.9626\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1180 - acc: 0.9678 - val_loss: 0.1292 - val_acc: 0.9623\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1171 - acc: 0.9680 - val_loss: 0.1289 - val_acc: 0.9629\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1161 - acc: 0.9681 - val_loss: 0.1277 - val_acc: 0.9624\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1152 - acc: 0.9688 - val_loss: 0.1275 - val_acc: 0.9636\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1144 - acc: 0.9686 - val_loss: 0.1271 - val_acc: 0.9631\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1135 - acc: 0.9690 - val_loss: 0.1261 - val_acc: 0.9633\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1126 - acc: 0.9691 - val_loss: 0.1255 - val_acc: 0.9638\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1118 - acc: 0.9695 - val_loss: 0.1250 - val_acc: 0.9641\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1110 - acc: 0.9695 - val_loss: 0.1239 - val_acc: 0.9651\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1102 - acc: 0.9701 - val_loss: 0.1233 - val_acc: 0.9639\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1093 - acc: 0.9700 - val_loss: 0.1229 - val_acc: 0.9649\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1085 - acc: 0.9703 - val_loss: 0.1229 - val_acc: 0.9643\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1078 - acc: 0.9704 - val_loss: 0.1222 - val_acc: 0.9651\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1069 - acc: 0.9709 - val_loss: 0.1213 - val_acc: 0.9649\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1062 - acc: 0.9708 - val_loss: 0.1206 - val_acc: 0.9653\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1056 - acc: 0.9707 - val_loss: 0.1202 - val_acc: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13369cb38>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9600/10000 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12017810336314141, 0.96550000000000002]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 96% accuracy after first 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1048 - acc: 0.9712 - val_loss: 0.1200 - val_acc: 0.9651\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1040 - acc: 0.9714 - val_loss: 0.1196 - val_acc: 0.9661\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1033 - acc: 0.9715 - val_loss: 0.1185 - val_acc: 0.9661\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1026 - acc: 0.9718 - val_loss: 0.1188 - val_acc: 0.9663\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1019 - acc: 0.9721 - val_loss: 0.1180 - val_acc: 0.9664\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1012 - acc: 0.9720 - val_loss: 0.1177 - val_acc: 0.9660\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1005 - acc: 0.9727 - val_loss: 0.1175 - val_acc: 0.9665\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0999 - acc: 0.9725 - val_loss: 0.1166 - val_acc: 0.9668\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0993 - acc: 0.9729 - val_loss: 0.1163 - val_acc: 0.9666\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0986 - acc: 0.9729 - val_loss: 0.1153 - val_acc: 0.9674\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0979 - acc: 0.9733 - val_loss: 0.1156 - val_acc: 0.9662\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0974 - acc: 0.9731 - val_loss: 0.1146 - val_acc: 0.9671\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0968 - acc: 0.9735 - val_loss: 0.1143 - val_acc: 0.9679\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0961 - acc: 0.9735 - val_loss: 0.1141 - val_acc: 0.9674\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0955 - acc: 0.9737 - val_loss: 0.1148 - val_acc: 0.9671\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0950 - acc: 0.9740 - val_loss: 0.1133 - val_acc: 0.9681\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0944 - acc: 0.9741 - val_loss: 0.1132 - val_acc: 0.9681\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0937 - acc: 0.9741 - val_loss: 0.1127 - val_acc: 0.9681\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0932 - acc: 0.9746 - val_loss: 0.1124 - val_acc: 0.9678\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0926 - acc: 0.9745 - val_loss: 0.1124 - val_acc: 0.9675\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0920 - acc: 0.9748 - val_loss: 0.1121 - val_acc: 0.9681\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0915 - acc: 0.9750 - val_loss: 0.1119 - val_acc: 0.9680\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0910 - acc: 0.9751 - val_loss: 0.1113 - val_acc: 0.9684\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0904 - acc: 0.9752 - val_loss: 0.1106 - val_acc: 0.9687\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0899 - acc: 0.9753 - val_loss: 0.1107 - val_acc: 0.9680\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0894 - acc: 0.9754 - val_loss: 0.1098 - val_acc: 0.9684\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0889 - acc: 0.9754 - val_loss: 0.1095 - val_acc: 0.9687\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0883 - acc: 0.9757 - val_loss: 0.1091 - val_acc: 0.9689\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0878 - acc: 0.9759 - val_loss: 0.1091 - val_acc: 0.9687\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0874 - acc: 0.9759 - val_loss: 0.1087 - val_acc: 0.9690\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0868 - acc: 0.9761 - val_loss: 0.1082 - val_acc: 0.9692\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0863 - acc: 0.9764 - val_loss: 0.1081 - val_acc: 0.9693\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0858 - acc: 0.9764 - val_loss: 0.1083 - val_acc: 0.9692\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0854 - acc: 0.9766 - val_loss: 0.1081 - val_acc: 0.9687\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0849 - acc: 0.9768 - val_loss: 0.1069 - val_acc: 0.9694\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0844 - acc: 0.9770 - val_loss: 0.1066 - val_acc: 0.9692\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0839 - acc: 0.9769 - val_loss: 0.1075 - val_acc: 0.9690\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0835 - acc: 0.9772 - val_loss: 0.1066 - val_acc: 0.9695\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0829 - acc: 0.9772 - val_loss: 0.1067 - val_acc: 0.9690\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0826 - acc: 0.9774 - val_loss: 0.1059 - val_acc: 0.9691\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0821 - acc: 0.9774 - val_loss: 0.1058 - val_acc: 0.9694\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0817 - acc: 0.9777 - val_loss: 0.1049 - val_acc: 0.9697\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0812 - acc: 0.9777 - val_loss: 0.1051 - val_acc: 0.9696\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0808 - acc: 0.9779 - val_loss: 0.1047 - val_acc: 0.9700\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0803 - acc: 0.9779 - val_loss: 0.1043 - val_acc: 0.9702\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0799 - acc: 0.9780 - val_loss: 0.1048 - val_acc: 0.9695\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0795 - acc: 0.9782 - val_loss: 0.1045 - val_acc: 0.9696\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0790 - acc: 0.9780 - val_loss: 0.1042 - val_acc: 0.9697\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0786 - acc: 0.9786 - val_loss: 0.1038 - val_acc: 0.9698\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0782 - acc: 0.9783 - val_loss: 0.1032 - val_acc: 0.9699\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0778 - acc: 0.9788 - val_loss: 0.1036 - val_acc: 0.9697\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0774 - acc: 0.9788 - val_loss: 0.1036 - val_acc: 0.9697\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0770 - acc: 0.9791 - val_loss: 0.1033 - val_acc: 0.9700\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0766 - acc: 0.9789 - val_loss: 0.1026 - val_acc: 0.9701\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0763 - acc: 0.9791 - val_loss: 0.1030 - val_acc: 0.9702\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0758 - acc: 0.9793 - val_loss: 0.1021 - val_acc: 0.9705\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0755 - acc: 0.9794 - val_loss: 0.1022 - val_acc: 0.9701\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0751 - acc: 0.9796 - val_loss: 0.1018 - val_acc: 0.9706\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0747 - acc: 0.9795 - val_loss: 0.1015 - val_acc: 0.9705\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0744 - acc: 0.9795 - val_loss: 0.1016 - val_acc: 0.9705\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0739 - acc: 0.9798 - val_loss: 0.1013 - val_acc: 0.9700\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0736 - acc: 0.9799 - val_loss: 0.1013 - val_acc: 0.9703\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0732 - acc: 0.9800 - val_loss: 0.1012 - val_acc: 0.9710\n",
      "Epoch 64/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0729 - acc: 0.9799 - val_loss: 0.1008 - val_acc: 0.9707\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0725 - acc: 0.9802 - val_loss: 0.1008 - val_acc: 0.9707\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0722 - acc: 0.9804 - val_loss: 0.1005 - val_acc: 0.9703\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0718 - acc: 0.9803 - val_loss: 0.1003 - val_acc: 0.9706\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0715 - acc: 0.9805 - val_loss: 0.0998 - val_acc: 0.9707\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0711 - acc: 0.9805 - val_loss: 0.1000 - val_acc: 0.9708\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0708 - acc: 0.9808 - val_loss: 0.0998 - val_acc: 0.9714\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0705 - acc: 0.9808 - val_loss: 0.0994 - val_acc: 0.9712\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0701 - acc: 0.9809 - val_loss: 0.0993 - val_acc: 0.9713\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0698 - acc: 0.9811 - val_loss: 0.0997 - val_acc: 0.9712\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0695 - acc: 0.9810 - val_loss: 0.0994 - val_acc: 0.9712\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0691 - acc: 0.9811 - val_loss: 0.0989 - val_acc: 0.9709\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0688 - acc: 0.9812 - val_loss: 0.0987 - val_acc: 0.9715\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0685 - acc: 0.9811 - val_loss: 0.0981 - val_acc: 0.9708\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0682 - acc: 0.9816 - val_loss: 0.0985 - val_acc: 0.9709\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0679 - acc: 0.9816 - val_loss: 0.0985 - val_acc: 0.9707\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0676 - acc: 0.9818 - val_loss: 0.0980 - val_acc: 0.9714\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0673 - acc: 0.9819 - val_loss: 0.0980 - val_acc: 0.9714\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0669 - acc: 0.9819 - val_loss: 0.0976 - val_acc: 0.9711\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0666 - acc: 0.9818 - val_loss: 0.0978 - val_acc: 0.9709\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0663 - acc: 0.9820 - val_loss: 0.0984 - val_acc: 0.9712\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0660 - acc: 0.9820 - val_loss: 0.0972 - val_acc: 0.9712\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0657 - acc: 0.9822 - val_loss: 0.0980 - val_acc: 0.9712\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0655 - acc: 0.9824 - val_loss: 0.0980 - val_acc: 0.9713\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0652 - acc: 0.9825 - val_loss: 0.0974 - val_acc: 0.9713\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0649 - acc: 0.9824 - val_loss: 0.0970 - val_acc: 0.9713\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0645 - acc: 0.9828 - val_loss: 0.0966 - val_acc: 0.9714\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0643 - acc: 0.9826 - val_loss: 0.0964 - val_acc: 0.9718\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0640 - acc: 0.9829 - val_loss: 0.0966 - val_acc: 0.9712\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0638 - acc: 0.9828 - val_loss: 0.0965 - val_acc: 0.9716\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0634 - acc: 0.9829 - val_loss: 0.0964 - val_acc: 0.9711\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0632 - acc: 0.9828 - val_loss: 0.0963 - val_acc: 0.9718\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0630 - acc: 0.9830 - val_loss: 0.0959 - val_acc: 0.9717\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.0958 - val_acc: 0.9717\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0624 - acc: 0.9832 - val_loss: 0.0955 - val_acc: 0.9722\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0621 - acc: 0.9830 - val_loss: 0.0958 - val_acc: 0.9719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1336c7f28>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8288/10000 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.095787641709297891, 0.97189999999999999]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 97% accuracy after another 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "In machine learning, a **convolutional neural network** (**CNN**, or\n",
    "**ConvNet**) is a class of deep, feed-forward artificial neural\n",
    "networks that has successfully been applied to analyzing visual\n",
    "imagery.\n",
    "\n",
    "CNNs use a variation of multilayer perceptrons designed to require\n",
    "minimal preprocessing. They are also known as **shift invariant**\n",
    "or **space invariant artificial neural networks** (**SIANN**), based on\n",
    "their shared-weights architecture and translation invariance\n",
    "characteristics. \n",
    "\n",
    "Convolutional networks were inspired by biological processes^4 in\n",
    "that the connectivity pattern between neurons resembles the\n",
    "organization of the animal visual cortex. Individual cortical\n",
    "neurons respond to stimuli only in a restricted region of the visual\n",
    "field known as the receptive field. The receptive fields of different\n",
    "neurons partially overlap such that they cover the entire visual field.\n",
    "\n",
    "![Convolutional Networks](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Convolutional_Networks.png)\n",
    "\n",
    "Convolutional Networks [https://youtu.be/jajksuQW4mc](https://youtu.be/jajksuQW4mc)\n",
    "\n",
    "![Introduction to Deep Learning: What Are Convolutional Neural Networks?](http://nikbearbrown.com/YouTube/MachineLearning/IMG/What_Are_Convolutional_Neural_Networks.png )\n",
    "\n",
    "Introduction to Deep Learning: What Are Convolutional Neural Networks? [https://youtu.be/ixF5WNpTzCA](https://youtu.be/ixF5WNpTzCA)\n",
    "\n",
    "![MIT 6.S191 Lecture 3: Convolutional Neural Networks](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Convolutional_Neural_Networks.png )\n",
    "MIT 6.S191 Lecture 3: Convolutional Neural Networks [https://youtu.be/v5JvvbP0d44](https://youtu.be/v5JvvbP0d44)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.S191: Introduction to Deep Learning\n",
    "\n",
    "For further learning I suggest [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/).\n",
    "\n",
    "* [MIT 6.S191: Introduction to Deep Learning](https://youtu.be/JN6H4rQvwgY)   \n",
    "* [MIT 6.S191: Sequence Modeling with Neural Networks](https://youtu.be/CznICCPa63Q)   \n",
    "* [MIT 6.S191: Convolutional Neural Networks](https://youtu.be/NVH8EYPHi30)   \n",
    "* [MIT 6.S191: Deep Generative Modeling](https://youtu.be/JVb54xhEw6Y)   \n",
    "* [MIT 6.S191: Deep Reinforcement Learning](https://youtu.be/s5qqjyGiBdc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "With a fairly simple shallow net we've done fairly well classifying (97% accuracy after another 99 epochs with Relu and Cross-entropy) on the [MNIST](http://yann.lecun.com/exdb/mnist/)  handwritten digit classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update July 8, 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
